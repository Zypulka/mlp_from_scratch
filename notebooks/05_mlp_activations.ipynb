{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.253588Z",
     "start_time": "2025-04-08T10:04:07.544888Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.266363Z",
     "start_time": "2025-04-08T10:04:08.258132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(44)\n",
    "random.seed(44)"
   ],
   "id": "dc9fcdb0d5818a23",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.296935Z",
     "start_time": "2025-04-08T10:04:08.274890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class NeuralNetwork5:\n",
    "\n",
    "    def __init__(self, layer_sizes, activation='sigmoid', output_activation='linear', init_method='normal',classification=True, bias_sd = 0.01):\n",
    "        \"\"\"\n",
    "        Inicjalizuje sieć neuronową MLP.\n",
    "\n",
    "        :param layer_sizes: Lista określająca liczbę neuronów w kolejnych warstwach, np. [1, 5, 1]\n",
    "        :param activation: Funkcja aktywacji dla warstw ukrytych (domyślnie 'sigmoid')\n",
    "        :param output_activation: Funkcja aktywacji dla warstwy wyjściowej (domyślnie 'linear')\n",
    "        :param init_method: Metoda inicjalizacji wag, opcje:\n",
    "                            'normal'  - N(0,1) (domyślnie),\n",
    "                            'uniform' - U[0,1],\n",
    "                            'he'      - inicjalizacja He,\n",
    "                            'xavier'  - inicjalizacja Xavier.\n",
    "                            W przypadku nieznanej metody używana jest inicjalizacja normalna.\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.init_method = init_method\n",
    "        self.classification = classification\n",
    "        self.gradient_history = []\n",
    "\n",
    "        # Inicjalizacja parametrów (wag i biasów) dla każdej warstwy poza wejściową\n",
    "        self.params = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            n_in = layer_sizes[i]\n",
    "            n_out = layer_sizes[i+1]\n",
    "\n",
    "            if init_method == 'normal':\n",
    "                # Domyślna inicjalizacja: N(0,1)\n",
    "                W = np.random.randn(n_out, n_in)\n",
    "                b = np.random.randn(n_out, 1) * bias_sd\n",
    "            elif init_method == 'uniform':\n",
    "                # Inicjalizacja z U[0,1]\n",
    "                W = np.random.rand(n_out, n_in)\n",
    "                b = np.random.rand(n_out, 1) * bias_sd\n",
    "            elif init_method == 'he':\n",
    "                # Inicjalizacja He: dla warstw z ReLU lub jej wariantami\n",
    "                W = np.random.randn(n_out, n_in) * np.sqrt(2 / n_in)\n",
    "                b = np.random.randn(n_out, 1) * bias_sd\n",
    "            elif init_method == 'xavier':\n",
    "                # Inicjalizacja Xavier: dla warstw z sigmoid lub tanh\n",
    "                W   = np.random.randn(n_out, n_in) * np.sqrt(1 / (n_in + n_out))\n",
    "                b = np.random.randn(n_out, 1) * bias_sd\n",
    "            else:\n",
    "                # W przypadku nieznanej metody używamy domyślnej inicjalizacji N(0,1)\n",
    "                W = np.random.randn(n_out, n_in)\n",
    "                b = np.random.randn(n_out, 1) * bias_sd\n",
    "\n",
    "            self.params.append({'W': W, 'b': b})\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Funkcja aktywacji sigmoidalnej z ograniczeniem zakresu dla stabilności numerycznej.\n",
    "        \"\"\"\n",
    "        z = np.clip(z, -15, 15)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Funkcja aktywacji ReLU\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation function.\n",
    "        \"\"\"\n",
    "        exps = np.exp(x - np.max(x, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def linear(self, z):\n",
    "        \"\"\"\n",
    "        Funkcja aktywacji liniowej.\n",
    "        \"\"\"\n",
    "        return z\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def sigmoid_derivative(self, a):\n",
    "        # a = sigmoid(z), więc d/dz sigmoid = a * (1 - a)\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        # Dla ReLU kluczowe jest z (a = relu(z) nie wystarcza do odróżnienia punktów 0)\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def tanh_derivative(self, a):\n",
    "        # a = tanh(z), więc d/dz tanh(z) = 1 - a^2\n",
    "        return 1 - a**2\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Przeprowadza propagację w przód.\n",
    "\n",
    "        :param X: Dane wejściowe w postaci macierzy [D x N] (D - cechy, N - liczba próbek)\n",
    "        :return: Wynik propagacji (output sieci)\n",
    "        \"\"\"\n",
    "        # Propagacja przez warstwy ukryte\n",
    "\n",
    "        activation_func = getattr(self, self.activation)\n",
    "        for i in range(len(self.layer_sizes) - 2):\n",
    "            X = activation_func(self.params[i]['W'] @ X + self.params[i]['b'])\n",
    "\n",
    "        # Warstwa wyjściowa z określoną funkcją aktywacji\n",
    "        output_func = getattr(self, self.output_activation)\n",
    "        return output_func(self.params[-1]['W'] @ X + self.params[-1]['b'])\n",
    "\n",
    "    def compute_mse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Oblicza funkcję kosztu (MSE).\n",
    "\n",
    "        MSE = (1/N) * sum((y_pred - y_true)^2)\n",
    "        \"\"\"\n",
    "        y_true = y_true.flatten()\n",
    "        y_pred = y_pred.flatten()\n",
    "        N = y_true.shape[0]\n",
    "        return (1 / N) * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "    def compute_cross_entropy(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss between the true and predicted values.\n",
    "        :param y: True values\n",
    "        :param y_pred: Predicted values\n",
    "        :return: The computed cross-entropy loss\n",
    "        \"\"\"\n",
    "        n = y.shape[0]\n",
    "        return -np.sum(y * np.log(y_pred)) / n\n",
    "\n",
    "\n",
    "    def calculate_gradient(self, x_batch, y_batch, clip_threshold=1.0):\n",
    "        \"\"\"\n",
    "        Oblicza gradienty wag i biasów metodą backpropagation z gradient clipping.\n",
    "\n",
    "        :param x_batch: Dane wejściowe [D x N], D - liczba cech, N - liczba próbek\n",
    "        :param y_batch: Odpowiedzi dla batcha (wektor lub macierz [output_size x N])\n",
    "        :param clip_threshold: Próg normy, powyżej którego gradienty są skalowane\n",
    "        :return: Lista gradientów dla poszczególnych warstw,\n",
    "                 gdzie każdy element to słownik {'W': dW, 'b': db}\n",
    "        \"\"\"\n",
    "        # ----------------------\n",
    "        # Forward pass\n",
    "        # ----------------------\n",
    "        activations = [x_batch]  # a^(0) = x_batch\n",
    "        zs = []                  # z = W*a + b dla każdej warstwy\n",
    "\n",
    "        num_layers = len(self.params)\n",
    "        for i in range(num_layers):\n",
    "            W = self.params[i]['W']\n",
    "            b = self.params[i]['b']\n",
    "\n",
    "            z = W @ activations[-1] + b\n",
    "            zs.append(z)\n",
    "\n",
    "            if i < num_layers - 1:\n",
    "                # warstwy ukryte\n",
    "                a = getattr(self, self.activation)(z)\n",
    "            else:\n",
    "                # warstwa wyjściowa\n",
    "                a = getattr(self, self.output_activation)(z)\n",
    "\n",
    "            activations.append(a)\n",
    "\n",
    "        # Upewnij się, że y_batch ma taki sam kształt jak output sieci\n",
    "        if y_batch.ndim == 1:\n",
    "            y_batch = y_batch.reshape(activations[-1].shape)\n",
    "\n",
    "        m = x_batch.shape[1]  # liczba próbek w batchu\n",
    "\n",
    "        # ----------------------\n",
    "        # Obliczenie \"delta\" od warstwy wyjściowej\n",
    "        # ----------------------\n",
    "        # Przykładowo: MSE -> d/dA_last = (2/m) * (A_last - y_batch)\n",
    "        delta = (2.0 / m) * (activations[-1] - y_batch)\n",
    "\n",
    "        # Modyfikacja delta przez pochodną funkcji aktywacji warstwy wyjściowej\n",
    "        if self.output_activation == 'linear':\n",
    "            # d/dz linear = 1 -> bez zmian\n",
    "            pass\n",
    "        elif self.output_activation == 'sigmoid':\n",
    "            delta *= self.sigmoid_derivative(activations[-1])\n",
    "        elif self.output_activation == 'relu':\n",
    "            delta *= self.relu_derivative(zs[-1])\n",
    "        elif self.output_activation == 'tanh':\n",
    "            delta *= self.tanh_derivative(activations[-1])\n",
    "        elif self.output_activation == 'softmax':\n",
    "            delta = (activations[-1] - y_batch)\n",
    "        else:\n",
    "            pass  # Jeśli inna funkcja, wstawić własny wariant\n",
    "\n",
    "        # ----------------------\n",
    "        # Backprop: warstwa wyjściowa\n",
    "        # ----------------------\n",
    "        gradients = [None] * num_layers\n",
    "\n",
    "        dW = delta @ activations[-2].T\n",
    "        db = np.sum(delta, axis=1, keepdims=True)\n",
    "        gradients[-1] = {'W': dW, 'b': db}\n",
    "\n",
    "        # ----------------------\n",
    "        # Backprop: warstwy ukryte\n",
    "        # ----------------------\n",
    "        for i in range(num_layers - 2, -1, -1):\n",
    "            # W_(i+1).T @ delta_(i+1)\n",
    "            delta = self.params[i+1]['W'].T @ delta\n",
    "\n",
    "            # Pochodna funkcji aktywacji warstwy ukrytej\n",
    "            if self.activation == 'sigmoid':\n",
    "                delta *= self.sigmoid_derivative(activations[i+1])\n",
    "            elif self.activation == 'relu':\n",
    "                delta *= self.relu_derivative(zs[i])\n",
    "            elif self.activation == 'tanh':\n",
    "                delta *= self.tanh_derivative(activations[i+1])\n",
    "            elif self.activation == 'linear':\n",
    "                pass\n",
    "\n",
    "            dW = delta @ activations[i].T\n",
    "            db = np.sum(delta, axis=1, keepdims=True)\n",
    "            gradients[i] = {'W': dW, 'b': db}\n",
    "\n",
    "        # ----------------------\n",
    "        # Gradient clipping\n",
    "        # ----------------------\n",
    "        for layer_grad in gradients:\n",
    "            for key in layer_grad:\n",
    "                grad_norm = np.linalg.norm(layer_grad[key])\n",
    "                if grad_norm > clip_threshold:\n",
    "                    layer_grad[key] *= (clip_threshold / grad_norm)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def vector_to_gradients(self, grad_vector):\n",
    "        \"\"\"\n",
    "        Odwraca funkcję gradients_to_vector.\n",
    "        Na podstawie wektora grad_vector oraz oryginalnych kształtów parametrów,\n",
    "        zwraca listę słowników gradientów w tej samej strukturze, co w self.params.\n",
    "\n",
    "        :param grad_vector: Jednowymiarowy numpy array zawierający wszystkie gradienty.\n",
    "        :return: Lista słowników gradientów, gdzie każdy słownik ma klucze 'W' oraz 'b'\n",
    "                 z gradientami o odpowiednich kształtach.\n",
    "        \"\"\"\n",
    "        gradients = []\n",
    "        current_index = 0\n",
    "\n",
    "        # Iterujemy przez warstwy korzystając z kształtów parametrów zapisanych w self.params\n",
    "        for layer in self.params:\n",
    "            layer_grad = {}\n",
    "\n",
    "            # Kształt i liczba elementów gradientu dla wag (W)\n",
    "            W_shape = layer['W'].shape\n",
    "            W_size = np.prod(W_shape)\n",
    "            # Wydzielamy fragment wektora dla wag i przekształcamy go do odpowiedniego kształtu\n",
    "            W_grad = grad_vector[current_index: current_index + W_size].reshape(W_shape)\n",
    "            current_index += W_size\n",
    "\n",
    "            # Kształt i liczba elementów gradientu dla biasów (b)\n",
    "            b_shape = layer['b'].shape\n",
    "            b_size = np.prod(b_shape)\n",
    "            # Wydzielamy fragment wektora dla biasów i przekształcamy go do odpowiedniego kształtu\n",
    "            b_grad = grad_vector[current_index: current_index + b_size].reshape(b_shape)\n",
    "            current_index += b_size\n",
    "\n",
    "            layer_grad['W'] = W_grad\n",
    "            layer_grad['b'] = b_grad\n",
    "\n",
    "            gradients.append(layer_grad)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def gradients_to_vector(self,gradients):\n",
    "        \"\"\"\n",
    "        Zamienia listę słowników gradientów na jeden wektor.\n",
    "\n",
    "        :param gradients: Lista słowników gradientów,\n",
    "                          gdzie każdy słownik ma klucze 'W' (gradient wag)\n",
    "                          oraz 'b' (gradient biasów).\n",
    "        :return: Jednowymiarowy numpy array zawierający wszystkie gradienty.\n",
    "        \"\"\"\n",
    "        grad_vector = []\n",
    "        for layer_grad in gradients:\n",
    "            # Spłaszczamy gradient wag i biasów i dodajemy do listy\n",
    "            grad_vector.append(layer_grad['W'].ravel())\n",
    "            grad_vector.append(layer_grad['b'].ravel())\n",
    "        # Łączymy wszystkie spłaszczone elementy w jeden wektor\n",
    "        return np.concatenate(grad_vector)\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "              X_train,\n",
    "              y_train,\n",
    "              batch_size,\n",
    "              epochs,\n",
    "              learning_rate=0.01,\n",
    "              verbose=250,\n",
    "              optimizer='basic',\n",
    "              beta=0.9,\n",
    "              beta_1=0.9,\n",
    "              beta_2=0.999,\n",
    "              eps=1e-8,\n",
    "              return_loss = True\n",
    "              ):\n",
    "        \"\"\"\n",
    "        Trenuje sieć neuronową metodą mini-batch gradient descent, wspierając\n",
    "        różne optymalizatory (m.in. Adam, RMSProp).\n",
    "\n",
    "        Parametry:\n",
    "        ----------\n",
    "        X_train : ndarray, shape [D, N]\n",
    "            Dane treningowe, gdzie D to liczba cech, a N to liczba próbek.\n",
    "        y_train : ndarray, shape [N] lub [1, N]\n",
    "            Odpowiedzi (etykiety) dla próbek.\n",
    "        batch_size : int\n",
    "            Rozmiar mini-batcha.\n",
    "        epochs : int\n",
    "            Liczba epok treningowych.\n",
    "        learning_rate : float\n",
    "            Współczynnik uczenia (domyślnie 0.01).\n",
    "        verbose : int\n",
    "            Co ile epok wyświetlać informację o błędzie (domyślnie co 250).\n",
    "        optimizer : {'basic', 'adam', 'RMSProp'}\n",
    "            Wybór optymalizatora.\n",
    "        beta : float\n",
    "            Współczynnik momentum dla RMSProp (domyślnie 0.9).\n",
    "        beta_1 : float\n",
    "            Współczynnik momentum dla Adama (domyślnie 0.9).\n",
    "        beta_2 : float\n",
    "            Współczynnik dla średniej kwadratów gradientów w Adamie (domyślnie 0.999).\n",
    "        eps : float\n",
    "            Drobna stała zapobiegająca dzieleniu przez zero (domyślnie 1e-8).\n",
    "        return_losses : bool\n",
    "            Jeśli True, po zakończeniu treningu zwracana jest lista strat\n",
    "            z każdej epoki (domyślnie False).\n",
    "\n",
    "        Zwraca:\n",
    "        -------\n",
    "        Nic (lub listę strat, jeśli return_losses=True).\n",
    "        \"\"\"\n",
    "\n",
    "        # Zmienna 't' dla Adama – zlicza łączną liczbę batchy (kroków optymalizacji).\n",
    "        if optimizer == 'adam':\n",
    "            t = 0\n",
    "\n",
    "\n",
    "        # Liczba próbek\n",
    "        num_samples = X_train.shape[1]\n",
    "\n",
    "        # Pętla po epokach\n",
    "        for epoch in range(epochs):\n",
    "            # Losowa permutacja indeksów (shuffle)\n",
    "            indices = np.random.permutation(num_samples)\n",
    "\n",
    "            # Pętla po batchach\n",
    "            for start_idx in range(0, num_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, num_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "                # Tworzymy batch\n",
    "                X_batch = X_train[:, batch_indices]\n",
    "\n",
    "\n",
    "                if self.classification:\n",
    "                    y_batch = y_train[:,batch_indices]\n",
    "                else:\n",
    "                    y_batch = y_train[batch_indices]\n",
    "\n",
    "\n",
    "                # Obliczamy gradienty (backprop + ewentualny clipping)\n",
    "                gradients = self.calculate_gradient(X_batch, y_batch)\n",
    "\n",
    "                # Jeśli używamy Adama lub RMSProp, przetwarzamy te gradienty\n",
    "                if optimizer in ('adam', 'RMSProp'):\n",
    "                    # Inkrementacja kroków optymalizacji\n",
    "                    vector_gradient = self.gradients_to_vector(gradients)\n",
    "\n",
    "                    if optimizer == 'adam':\n",
    "                        t += 1\n",
    "                        # Inicjalizacja m_t i v_t w pierwszym kroku\n",
    "                        if epoch == 0 and start_idx == 0:\n",
    "                            m_t = np.zeros_like(vector_gradient)\n",
    "                            v_t = np.zeros_like(vector_gradient)\n",
    "                        else:\n",
    "                            # Aktualizacja pierwszego i drugiego momentu\n",
    "                            m_t = beta_1 * m_t + (1 - beta_1) * vector_gradient\n",
    "                            v_t = beta_2 * v_t + (1 - beta_2) * (vector_gradient ** 2)\n",
    "\n",
    "                        # Korekta biasu (Adam)\n",
    "                        m_t_hat = m_t / (1 - beta_1 ** t)\n",
    "                        v_t_hat = v_t / (1 - beta_2 ** t)\n",
    "\n",
    "                        # Obliczamy przyrost (update)\n",
    "                        update = m_t_hat / (np.sqrt(v_t_hat) + eps)\n",
    "\n",
    "                        # Zamiana wektora 'update' w strukturę gradientów\n",
    "                        gradients = self.vector_to_gradients(update)\n",
    "\n",
    "                    elif optimizer == 'RMSProp':\n",
    "                        # Inicjalizacja w pierwszym kroku\n",
    "                        if epoch == 0 and start_idx == 0:\n",
    "                            gradient_squared_mean = np.zeros_like(vector_gradient)\n",
    "\n",
    "                        # Aktualizacja wykładniczej średniej kwadratów gradientów\n",
    "                        gradient_squared_mean = beta * gradient_squared_mean + (1 - beta) * (vector_gradient ** 2)\n",
    "\n",
    "                        # Obliczamy przyrost\n",
    "                        update = vector_gradient / (np.sqrt(gradient_squared_mean) + eps)\n",
    "\n",
    "                        # Konwersja wektora 'update' na listę gradientów\n",
    "                        gradients = self.vector_to_gradients(update)\n",
    "\n",
    "                # Aktualizacja wag i biasów w sieci\n",
    "                for i in range(len(self.params)):\n",
    "                    self.params[i]['W'] -= learning_rate * gradients[i]['W']\n",
    "                    self.params[i]['b'] -= learning_rate * gradients[i]['b']\n",
    "\n",
    "            # Wyświetlamy, jeśli przypada kolej epoki zgodnie z verbose\n",
    "            if epoch % verbose == verbose-1:\n",
    "                y_pred = self.forward(X_train)\n",
    "                if self.classification:\n",
    "                    loss = self.compute_cross_entropy(y_train, y_pred)\n",
    "                    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "                else:\n",
    "                    mse = self.compute_mse(y_train, y_pred)\n",
    "                    print(f\"Epoch {epoch + 1}/{epochs}, MSE: {mse}\")\n",
    "\n",
    "\n",
    "        if self.classification:\n",
    "            final_loss = self.compute_cross_entropy(y_train, self.forward(X_train))\n",
    "            print(f\"Final loss: {final_loss}\")\n",
    "            if return_loss:\n",
    "                return final_loss\n",
    "        else:\n",
    "            final_mse = self.compute_mse(y_train, self.forward(X_train))\n",
    "            print(f\"Final MSE: {final_mse}\")\n",
    "            if return_loss:\n",
    "                return final_mse\n"
   ],
   "id": "b8617c857e07462f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.755776Z",
     "start_time": "2025-04-08T10:04:08.751592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    This function one hot encodes the labels\n",
    "    :param y: ndarray containing the labels\n",
    "    :return: ndarray with the one hot encoded labels\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot.T"
   ],
   "id": "153c1c2e7807b1d9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.767947Z",
     "start_time": "2025-04-08T10:04:08.762411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_and_scale_data_regression(training_csv, test_csv, feature_col='x', target_col='y'):\n",
    "    \"\"\"\n",
    "    Wczytuje dane z plików CSV oraz skaluje je (standaryzacja).\n",
    "\n",
    "    Parametry:\n",
    "      - training_csv: Ścieżka do pliku CSV z danymi treningowymi.\n",
    "      - test_csv: Ścieżka do pliku CSV z danymi testowymi.\n",
    "      - feature_col: Nazwa kolumny zawierającej cechy (domyślnie 'x').\n",
    "      - target_col: Nazwa kolumny zawierającej etykiety (domyślnie 'y').\n",
    "\n",
    "    Zwraca:\n",
    "      - x_train_scaled: Zescalowane dane treningowe (features) [1 x N_train].\n",
    "      - y_train_scaled: Zescalone etykiety treningowe.\n",
    "      - x_test_scaled: Zescalowane dane testowe (features) [1 x N_test].\n",
    "      - y_test: Oryginalne etykiety testowe (bez skalowania).\n",
    "      - scaling_params: Krotka (x_mean, x_std, y_mean, y_std) – parametry skalowania,\n",
    "                        które później można użyć do odwrotnej transformacji wyników.\n",
    "    \"\"\"\n",
    "    # Wczytanie danych\n",
    "\n",
    "\n",
    "    # Wyodrębnienie kolumn i przekształcenie do odpowiednich kształtów\n",
    "    x_train = training_csv[[feature_col]].values.T\n",
    "    y_train = training_csv[target_col].values\n",
    "\n",
    "    x_test = test_csv[[feature_col]].values.T\n",
    "    y_test = test_csv[target_col].values\n",
    "\n",
    "    # Obliczanie średniej i odchylenia standardowego dla x oraz y\n",
    "    x_mean = np.mean(x_train)\n",
    "    x_std  = np.std(x_train)\n",
    "    y_mean = np.mean(y_train)\n",
    "    y_std  = np.std(y_train)\n",
    "\n",
    "    # Skalowanie danych (standaryzacja)\n",
    "    x_train_scaled = (x_train - x_mean) / x_std\n",
    "    y_train_scaled = (y_train - y_mean) / y_std\n",
    "    x_test_scaled  = (x_test - x_mean) / x_std\n",
    "\n",
    "    scaling_params = (x_mean, x_std, y_mean, y_std)\n",
    "    return x_train_scaled, y_train_scaled, x_test_scaled, y_test, scaling_params"
   ],
   "id": "b1277f5c8dcd2637",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.837198Z",
     "start_time": "2025-04-08T10:04:08.832073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_scale_data_for_classification(training_data, test_data, target_col='c'):\n",
    "    \"\"\"\n",
    "    This function prepares the data for training and testing. It standardizes the features and one hot encodes the labels\n",
    "    :param training_data: DataFrame containing training data.\n",
    "    :param test_data: DataFrame containing test data.\n",
    "    :param target_col: string, name of the column containing labels. Defaults to 'c'.\n",
    "    :return: A tuple containing:\n",
    "        - x_train_scaled (ndarray): Standardized training features [1 x N_train].\n",
    "        - y_train (ndarray): One hot encoded training labels.\n",
    "        - x_test_scaled (ndarray): Standardized test features [1 x N_test].\n",
    "        - y_test (ndarray): Original test labels (not scaled).\n",
    "    \"\"\"\n",
    "    x_train = training_data.drop(columns=[target_col]).values.T\n",
    "    y_train = training_data[target_col].values\n",
    "    x_test = test_data.drop(columns=[target_col]).values.T\n",
    "    y_test = test_data[target_col].values\n",
    "    x_mean = np.mean(x_train)\n",
    "    x_std = np.std(x_train)\n",
    "    x_train_scaled = (x_train - x_mean) / x_std\n",
    "    x_test_scaled = (x_test - x_mean) / x_std\n",
    "    if y_train.dtype == bool:\n",
    "        y_train = y_train.astype(int)\n",
    "        y_test = y_test.astype(int)\n",
    "    y_train = one_hot_encode(y_train)\n",
    "    return x_train_scaled, y_train, x_test_scaled, y_test"
   ],
   "id": "4d2190c5310ce07c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.947885Z",
     "start_time": "2025-04-08T10:04:08.847779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wczytywanie multimodal-large\n",
    "multimodal_large_train = pd.read_csv(\"data/NN5/multimodal-large-training.csv\")\n",
    "multimodal_large_test  = pd.read_csv(\"data/NN5/multimodal-large-test.csv\")\n",
    "\n",
    "steps_large_train = pd.read_csv(\"data/NN5/steps-large-training.csv\")\n",
    "steps_large_test     = pd.read_csv(\"data/NN5/steps-large-test.csv\")\n",
    "\n",
    "rings3_train = pd.read_csv(\"data/NN5/rings3-regular-training.csv\")\n",
    "rings3_test = pd.read_csv(\"data/NN5/rings3-regular-test.csv\")\n",
    "\n",
    "rings5_train = pd.read_csv(\"data/NN5/rings5-regular-training.csv\")\n",
    "rings5_test = pd.read_csv(\"data/NN5/rings5-regular-test.csv\")\n"
   ],
   "id": "1e358c7c0bc897d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:04:08.998956Z",
     "start_time": "2025-04-08T10:04:08.993136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "def train_model_gridsearch(\n",
    "        dfs,\n",
    "        architectures,\n",
    "        activations,\n",
    "        classification,\n",
    "        output_activation,\n",
    "        batch_size=100,\n",
    "        epochs=100,\n",
    "        learning_rate=0.05,\n",
    "        verbose=500,\n",
    "        optimizer='basic'\n",
    "):\n",
    "\n",
    "\n",
    "    if not classification:\n",
    "        x_train_scaled, y_train_scaled, *_ = load_and_scale_data_regression(\n",
    "            dfs[\"train\"], dfs[\"test\"], 'x', 'y')\n",
    "    else:\n",
    "        x_train_scaled, y_train_scaled, *_ = load_and_scale_data_for_classification(dfs['train'], dfs['test'], target_col='c')\n",
    "\n",
    "    # Przygotowanie ramki danych do wyników\n",
    "    results = pd.DataFrame(index=activations, columns=architectures.keys())\n",
    "\n",
    "    # Pętla treningowa\n",
    "    for activation, (arch_name, layer_sizes) in product(activations, architectures.items()):\n",
    "        losses = []\n",
    "        for _ in range(5):\n",
    "            model = NeuralNetwork5(\n",
    "                layer_sizes=layer_sizes,\n",
    "                activation=activation,\n",
    "                output_activation=output_activation,\n",
    "                init_method='normal',\n",
    "                classification=classification\n",
    "            )\n",
    "            loss = model.train(\n",
    "                x_train_scaled, y_train_scaled,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                verbose=verbose,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "            losses.append(loss)\n",
    "        results.loc[activation, arch_name] = np.mean(losses)\n",
    "\n",
    "    return results\n"
   ],
   "id": "15c6fbaf25f6d428",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# testy na multimodal",
   "id": "f4a716c2b869e915"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:07:09.183208Z",
     "start_time": "2025-04-08T10:04:09.014283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs_multimodal = {\n",
    "    \"train\": multimodal_large_train,\n",
    "    \"test\": multimodal_large_test\n",
    "}\n",
    "\n",
    "results_multimodal = train_model_gridsearch(\n",
    "    dfs=dfs_multimodal,\n",
    "    architectures = {\n",
    "        \"1-2-1\": [1, 2, 1],\n",
    "        \"1-4-1\": [1, 4, 1],\n",
    "        \"1-8-1\": [1, 8, 1],\n",
    "        \"1-2-2-1\": [1, 2, 2, 1],\n",
    "        \"1-4-4-1\": [1, 4, 4, 1],\n",
    "        \"1-8-8-1\": [1, 8, 8, 1],\n",
    "        \"1-2-2-2-1\": [1, 2, 2, 2, 1],\n",
    "        \"1-4-4-4-1\": [1, 4, 4, 4, 1],\n",
    "        \"1-8-8-8-1\": [1, 8, 8, 8, 1],\n",
    "        \"1-16-1\": [1, 16, 1],\n",
    "        \"1-16-16-1\": [1, 16, 16, 1],\n",
    "        \"1-16-16-16-1\": [1, 16, 16, 16, 1]\n",
    "    },\n",
    "    activations=[\"sigmoid\", \"relu\", \"tanh\", \"linear\"],\n",
    "    classification=False,\n",
    "    output_activation=\"linear\"\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "a1462e9b35632642",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE: 0.36179028791868917\n",
      "Final MSE: 0.3640700964903205\n",
      "Final MSE: 0.3613372440136941\n",
      "Final MSE: 0.3668197180242447\n",
      "Final MSE: 0.3607030631021684\n",
      "Final MSE: 0.30342944495256124\n",
      "Final MSE: 0.29874031680344193\n",
      "Final MSE: 0.36039267423698657\n",
      "Final MSE: 0.3598365442886046\n",
      "Final MSE: 0.3580559809508623\n",
      "Final MSE: 0.3383582093469374\n",
      "Final MSE: 0.24807677687933424\n",
      "Final MSE: 0.35972653605691823\n",
      "Final MSE: 0.3605600061777429\n",
      "Final MSE: 0.3392672807660635\n",
      "Final MSE: 0.3314142749285868\n",
      "Final MSE: 0.3177880636368762\n",
      "Final MSE: 0.43929063912633465\n",
      "Final MSE: 0.33396226126558365\n",
      "Final MSE: 0.5320497166965491\n",
      "Final MSE: 0.29442738556319575\n",
      "Final MSE: 0.27618275605697457\n",
      "Final MSE: 0.31829788556742905\n",
      "Final MSE: 0.3158345598998764\n",
      "Final MSE: 0.2948219736091145\n",
      "Final MSE: 0.15375618397560775\n",
      "Final MSE: 0.24729556030825461\n",
      "Final MSE: 0.2778574878367363\n",
      "Final MSE: 0.26700689658473536\n",
      "Final MSE: 0.276363949152779\n",
      "Final MSE: 0.3315717929598721\n",
      "Final MSE: 0.5075491966579554\n",
      "Final MSE: 0.3206154477592773\n",
      "Final MSE: 0.3482133261022622\n",
      "Final MSE: 0.4978789199971164\n",
      "Final MSE: 0.30241140282387824\n",
      "Final MSE: 0.19731686703896337\n",
      "Final MSE: 0.31193130560361637\n",
      "Final MSE: 0.3158718478807138\n",
      "Final MSE: 0.30631405611137497\n",
      "Final MSE: 0.26872035741248823\n",
      "Final MSE: 0.2977218007758455\n",
      "Final MSE: 0.3148836132300468\n",
      "Final MSE: 0.12722918038484943\n",
      "Final MSE: 0.3106020567206519\n",
      "Final MSE: 0.36457998121076934\n",
      "Final MSE: 0.36523964662775105\n",
      "Final MSE: 0.18580460855936784\n",
      "Final MSE: 0.27913135021937374\n",
      "Final MSE: 0.35878217174771265\n",
      "Final MSE: 0.19974021375786422\n",
      "Final MSE: 0.10161124042382647\n",
      "Final MSE: 0.1961511268456113\n",
      "Final MSE: 0.12227095802012287\n",
      "Final MSE: 0.10147012152529677\n",
      "Final MSE: 0.08717312753589722\n",
      "Final MSE: 0.09232742290914933\n",
      "Final MSE: 0.08637306557527794\n",
      "Final MSE: 0.10364712071159853\n",
      "Final MSE: 0.08225407043502185\n",
      "Final MSE: 0.9292993456039927\n",
      "Final MSE: 0.5251351144951355\n",
      "Final MSE: 0.5296820210142885\n",
      "Final MSE: 0.930054280928385\n",
      "Final MSE: 0.5250092537469966\n",
      "Final MSE: 0.37999680645458234\n",
      "Final MSE: 0.5160085252802074\n",
      "Final MSE: 0.39608052329943877\n",
      "Final MSE: 0.5252360987018103\n",
      "Final MSE: 0.5319101428564793\n",
      "Final MSE: 0.5106785597893211\n",
      "Final MSE: 0.3292616059791965\n",
      "Final MSE: 0.2465682229199854\n",
      "Final MSE: 0.3218505993681888\n",
      "Final MSE: 0.26286322907108933\n",
      "Final MSE: 0.6197134228314787\n",
      "Final MSE: 1.000614163123102\n",
      "Final MSE: 0.28021213863217925\n",
      "Final MSE: 0.33152383383495626\n",
      "Final MSE: 0.9288773983803192\n",
      "Final MSE: 0.07794392917393439\n",
      "Final MSE: 0.31024728719041655\n",
      "Final MSE: 0.25086818640124775\n",
      "Final MSE: 0.18272744589017662\n",
      "Final MSE: 0.2984881942296567\n",
      "Final MSE: 0.04623025603600013\n",
      "Final MSE: 0.0842633218245779\n",
      "Final MSE: 0.060528400280218796\n",
      "Final MSE: 0.04170033374798831\n",
      "Final MSE: 0.0732513342948521\n",
      "Final MSE: 1.0007924432247877\n",
      "Final MSE: 0.49025461650012897\n",
      "Final MSE: 1.0000571151018736\n",
      "Final MSE: 0.9288828264490671\n",
      "Final MSE: 0.2938475067091647\n",
      "Final MSE: 0.08472089713204148\n",
      "Final MSE: 0.09616358273027309\n",
      "Final MSE: 0.2749565194464943\n",
      "Final MSE: 0.26818143479161904\n",
      "Final MSE: 0.2494485193188829\n",
      "Final MSE: 0.03763381151109021\n",
      "Final MSE: 0.030547505725178146\n",
      "Final MSE: 0.049070860015893725\n",
      "Final MSE: 0.026841498240890035\n",
      "Final MSE: 0.016378270011128602\n",
      "Final MSE: 0.35806821514237064\n",
      "Final MSE: 0.17327470775330506\n",
      "Final MSE: 0.18360938024124093\n",
      "Final MSE: 0.12529270062499392\n",
      "Final MSE: 0.3611524396272662\n",
      "Final MSE: 0.03248408560039109\n",
      "Final MSE: 0.06388624371676306\n",
      "Final MSE: 0.036402850710716986\n",
      "Final MSE: 0.03548855838793421\n",
      "Final MSE: 0.017070598303287762\n",
      "Final MSE: 0.026627539886022112\n",
      "Final MSE: 0.004065448692531255\n",
      "Final MSE: 0.009454126171705539\n",
      "Final MSE: 0.006758009510936453\n",
      "Final MSE: 0.0025128257952123636\n",
      "Final MSE: 0.3286603472422131\n",
      "Final MSE: 0.31566500876141435\n",
      "Final MSE: 0.32723308691614517\n",
      "Final MSE: 0.3277794281827514\n",
      "Final MSE: 0.3155173039446094\n",
      "Final MSE: 0.10315358911773218\n",
      "Final MSE: 0.1007808842922086\n",
      "Final MSE: 0.10283892598250394\n",
      "Final MSE: 0.10757682898930983\n",
      "Final MSE: 0.12082895066636884\n",
      "Final MSE: 0.09674481090286342\n",
      "Final MSE: 0.0861909386378027\n",
      "Final MSE: 0.09247167793451666\n",
      "Final MSE: 0.0841348718319477\n",
      "Final MSE: 0.0857377824212627\n",
      "Final MSE: 0.31405685614981843\n",
      "Final MSE: 0.3099330329753961\n",
      "Final MSE: 0.3103653094055107\n",
      "Final MSE: 0.22590682535035386\n",
      "Final MSE: 0.31388004024082944\n",
      "Final MSE: 0.08510515682263493\n",
      "Final MSE: 0.06398229832086168\n",
      "Final MSE: 0.06431492875305696\n",
      "Final MSE: 0.07062382827811592\n",
      "Final MSE: 0.07890455955157165\n",
      "Final MSE: 0.02499756415301575\n",
      "Final MSE: 0.024185648626750652\n",
      "Final MSE: 0.045377364185924675\n",
      "Final MSE: 0.05837740608319863\n",
      "Final MSE: 0.0768265244801007\n",
      "Final MSE: 0.29180321504571877\n",
      "Final MSE: 0.2920320157913182\n",
      "Final MSE: 0.12280538694301993\n",
      "Final MSE: 0.2015105633219084\n",
      "Final MSE: 0.26111135436780025\n",
      "Final MSE: 0.08141473469835193\n",
      "Final MSE: 0.06151365714939043\n",
      "Final MSE: 0.034506870986096644\n",
      "Final MSE: 0.2638102432648798\n",
      "Final MSE: 0.0787855050251899\n",
      "Final MSE: 0.01609648118312507\n",
      "Final MSE: 0.0022716577591850883\n",
      "Final MSE: 0.06067378155133954\n",
      "Final MSE: 0.03699490427655036\n",
      "Final MSE: 0.00846398190237058\n",
      "Final MSE: 0.06703244874993569\n",
      "Final MSE: 0.07106657481820776\n",
      "Final MSE: 0.07450417512248414\n",
      "Final MSE: 0.06446989152911042\n",
      "Final MSE: 0.07569230366255397\n",
      "Final MSE: 0.012481294713691186\n",
      "Final MSE: 0.012175769545957076\n",
      "Final MSE: 0.05080300980545106\n",
      "Final MSE: 0.008825802289626782\n",
      "Final MSE: 0.031617425552400276\n",
      "Final MSE: 0.0034530112459382673\n",
      "Final MSE: 0.028214234459972485\n",
      "Final MSE: 0.002920091658141583\n",
      "Final MSE: 0.0026444622262130277\n",
      "Final MSE: 0.00463493829036297\n",
      "Final MSE: 0.8497067564918298\n",
      "Final MSE: 0.8503731577632105\n",
      "Final MSE: 0.8521873289187565\n",
      "Final MSE: 0.8498553121235297\n",
      "Final MSE: 0.8516132563662212\n",
      "Final MSE: 0.8499003842275538\n",
      "Final MSE: 0.8496812802953085\n",
      "Final MSE: 0.851122599164202\n",
      "Final MSE: 0.8495380958465503\n",
      "Final MSE: 0.850301993133846\n",
      "Final MSE: 0.8496791567479768\n",
      "Final MSE: 0.8525077407215693\n",
      "Final MSE: 0.8495769734904948\n",
      "Final MSE: 0.8512323128529781\n",
      "Final MSE: 0.8505307135306239\n",
      "Final MSE: 0.8496771717436352\n",
      "Final MSE: 0.8502934906097362\n",
      "Final MSE: 0.8500352856962163\n",
      "Final MSE: 0.8495583736678349\n",
      "Final MSE: 0.8496869722617539\n",
      "Final MSE: 0.8496982336859142\n",
      "Final MSE: 0.8503603036115558\n",
      "Final MSE: 0.8496755065559077\n",
      "Final MSE: 0.8497497646915849\n",
      "Final MSE: 0.8509519258473675\n",
      "Final MSE: 0.8499531649555772\n",
      "Final MSE: 0.8501279581224939\n",
      "Final MSE: 0.8497516256741301\n",
      "Final MSE: 0.8510796374975315\n",
      "Final MSE: 0.8500440129495\n",
      "Final MSE: 0.8535403176090135\n",
      "Final MSE: 0.8519007742660226\n",
      "Final MSE: 0.8497720163117234\n",
      "Final MSE: 0.8496505461590489\n",
      "Final MSE: 0.8518982140584457\n",
      "Final MSE: 0.8524771081505166\n",
      "Final MSE: 0.853881954923861\n",
      "Final MSE: 0.8505337272074072\n",
      "Final MSE: 0.8509353291225791\n",
      "Final MSE: 0.8495761462065979\n",
      "Final MSE: 0.8512537071777486\n",
      "Final MSE: 0.8496710502023558\n",
      "Final MSE: 0.854929753940578\n",
      "Final MSE: 0.8521341788806982\n",
      "Final MSE: 0.8501001529808364\n",
      "Final MSE: 0.8523742814777209\n",
      "Final MSE: 0.8536911898324908\n",
      "Final MSE: 0.849862422704338\n",
      "Final MSE: 0.8495336330103601\n",
      "Final MSE: 0.852089813597659\n",
      "Final MSE: 0.8525406231456871\n",
      "Final MSE: 0.8498646553304472\n",
      "Final MSE: 0.8511258290981302\n",
      "Final MSE: 0.8497358789294182\n",
      "Final MSE: 0.850529599549873\n",
      "Final MSE: 0.8525629741589866\n",
      "Final MSE: 0.852242856662346\n",
      "Final MSE: 0.8551758183585855\n",
      "Final MSE: 0.8503209501600332\n",
      "Final MSE: 0.8505355245108047\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:07:09.310412Z",
     "start_time": "2025-04-08T10:07:09.292833Z"
    }
   },
   "cell_type": "code",
   "source": "results_multimodal",
   "id": "b9cc80b8c1c35ffe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            1-2-1     1-4-1     1-8-1   1-2-2-1   1-4-4-1   1-8-8-1 1-2-2-2-1  \\\n",
       "sigmoid  0.362944  0.336091  0.329198  0.390901  0.299913  0.244456  0.401166   \n",
       "relu     0.687836  0.469846  0.334244  0.632188  0.224055  0.061195  0.742767   \n",
       "tanh     0.322971  0.107036  0.089056  0.294828  0.072586  0.045953  0.233853   \n",
       "linear   0.850747  0.850109  0.850705   0.84985  0.850087  0.850191  0.851352   \n",
       "\n",
       "        1-4-4-4-1 1-8-8-8-1    1-16-1 1-16-16-1 1-16-16-16-1  \n",
       "sigmoid  0.286769  0.263831  0.310708  0.144249     0.090355  \n",
       "relu     0.194694  0.032094  0.240279  0.037066     0.009884  \n",
       "tanh     0.104006    0.0249  0.070553  0.023181     0.008373  \n",
       "linear   0.851481  0.851618   0.85151  0.850759     0.852168  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-2-1</th>\n",
       "      <th>1-4-1</th>\n",
       "      <th>1-8-1</th>\n",
       "      <th>1-2-2-1</th>\n",
       "      <th>1-4-4-1</th>\n",
       "      <th>1-8-8-1</th>\n",
       "      <th>1-2-2-2-1</th>\n",
       "      <th>1-4-4-4-1</th>\n",
       "      <th>1-8-8-8-1</th>\n",
       "      <th>1-16-1</th>\n",
       "      <th>1-16-16-1</th>\n",
       "      <th>1-16-16-16-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigmoid</th>\n",
       "      <td>0.362944</td>\n",
       "      <td>0.336091</td>\n",
       "      <td>0.329198</td>\n",
       "      <td>0.390901</td>\n",
       "      <td>0.299913</td>\n",
       "      <td>0.244456</td>\n",
       "      <td>0.401166</td>\n",
       "      <td>0.286769</td>\n",
       "      <td>0.263831</td>\n",
       "      <td>0.310708</td>\n",
       "      <td>0.144249</td>\n",
       "      <td>0.090355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>0.687836</td>\n",
       "      <td>0.469846</td>\n",
       "      <td>0.334244</td>\n",
       "      <td>0.632188</td>\n",
       "      <td>0.224055</td>\n",
       "      <td>0.061195</td>\n",
       "      <td>0.742767</td>\n",
       "      <td>0.194694</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>0.240279</td>\n",
       "      <td>0.037066</td>\n",
       "      <td>0.009884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>0.322971</td>\n",
       "      <td>0.107036</td>\n",
       "      <td>0.089056</td>\n",
       "      <td>0.294828</td>\n",
       "      <td>0.072586</td>\n",
       "      <td>0.045953</td>\n",
       "      <td>0.233853</td>\n",
       "      <td>0.104006</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.070553</td>\n",
       "      <td>0.023181</td>\n",
       "      <td>0.008373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.850747</td>\n",
       "      <td>0.850109</td>\n",
       "      <td>0.850705</td>\n",
       "      <td>0.84985</td>\n",
       "      <td>0.850087</td>\n",
       "      <td>0.850191</td>\n",
       "      <td>0.851352</td>\n",
       "      <td>0.851481</td>\n",
       "      <td>0.851618</td>\n",
       "      <td>0.85151</td>\n",
       "      <td>0.850759</td>\n",
       "      <td>0.852168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### wygrywa tanh/relu 1-16-16-16-1",
   "id": "fc0c356234d5ca34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# testy dla steps_large",
   "id": "137a6dc821cd1dc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:11:02.077385Z",
     "start_time": "2025-04-08T10:08:05.505956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs_steps_large = {\n",
    "    \"train\": steps_large_train,\n",
    "    \"test\": steps_large_test\n",
    "}\n",
    "\n",
    "results_steps_large = train_model_gridsearch(\n",
    "    dfs=dfs_steps_large,\n",
    "    architectures = {\n",
    "        \"1-2-1\": [1, 2, 1],\n",
    "        \"1-4-1\": [1, 4, 1],\n",
    "        \"1-8-1\": [1, 8, 1],\n",
    "        \"1-2-2-1\": [1, 2, 2, 1],\n",
    "        \"1-4-4-1\": [1, 4, 4, 1],\n",
    "        \"1-8-8-1\": [1, 8, 8, 1],\n",
    "        \"1-2-2-2-1\": [1, 2, 2, 2, 1],\n",
    "        \"1-4-4-4-1\": [1, 4, 4, 4, 1],\n",
    "        \"1-8-8-8-1\": [1, 8, 8, 8, 1],\n",
    "        \"1-16-1\": [1, 16, 1],\n",
    "        \"1-16-16-1\": [1, 16, 16, 1],\n",
    "        \"1-16-16-16-1\": [1, 16, 16, 16, 1]\n",
    "    },\n",
    "    activations=[\"sigmoid\", \"relu\", \"tanh\", \"linear\"],\n",
    "    classification=False,\n",
    "    output_activation=\"linear\"\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "dfe5f749a9417ad8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MSE: 0.07054156384255571\n",
      "Final MSE: 0.07053649144961667\n",
      "Final MSE: 0.0708278582844069\n",
      "Final MSE: 0.0704165317829581\n",
      "Final MSE: 0.07078960333156245\n",
      "Final MSE: 0.0706733910801956\n",
      "Final MSE: 0.07016648402117454\n",
      "Final MSE: 0.0701904804955443\n",
      "Final MSE: 0.07071001993280863\n",
      "Final MSE: 0.07035351014325458\n",
      "Final MSE: 0.07019310274983319\n",
      "Final MSE: 0.06996560517194886\n",
      "Final MSE: 0.06947177651141909\n",
      "Final MSE: 0.07045389892102294\n",
      "Final MSE: 0.07111766753744887\n",
      "Final MSE: 0.0713355612277263\n",
      "Final MSE: 0.07133280934690352\n",
      "Final MSE: 0.07177906608256886\n",
      "Final MSE: 0.07158525130160479\n",
      "Final MSE: 0.07140377165460204\n",
      "Final MSE: 0.07009496700372947\n",
      "Final MSE: 0.0709853014044366\n",
      "Final MSE: 0.07071996740665744\n",
      "Final MSE: 0.0700707624721753\n",
      "Final MSE: 0.07078367905210128\n",
      "Final MSE: 0.07090060874849141\n",
      "Final MSE: 0.0703901679831918\n",
      "Final MSE: 0.07159112400352569\n",
      "Final MSE: 0.0691881634382518\n",
      "Final MSE: 0.07046235753601222\n",
      "Final MSE: 0.07131300126228188\n",
      "Final MSE: 0.07226295771880721\n",
      "Final MSE: 0.0722183454114541\n",
      "Final MSE: 0.07155755078843107\n",
      "Final MSE: 0.07192159008457562\n",
      "Final MSE: 0.07001102047193294\n",
      "Final MSE: 0.07121279503772365\n",
      "Final MSE: 0.07259886736420072\n",
      "Final MSE: 0.07089412743424127\n",
      "Final MSE: 0.07061468920336432\n",
      "Final MSE: 0.06970659255849014\n",
      "Final MSE: 0.06975017041947928\n",
      "Final MSE: 0.07110785231105551\n",
      "Final MSE: 0.06863564200995118\n",
      "Final MSE: 0.06755596074347019\n",
      "Final MSE: 0.07051174368301083\n",
      "Final MSE: 0.06990823816507462\n",
      "Final MSE: 0.07095946513854846\n",
      "Final MSE: 0.07050470756601013\n",
      "Final MSE: 0.07056260235709835\n",
      "Final MSE: 0.06651744672525854\n",
      "Final MSE: 0.06570119170429749\n",
      "Final MSE: 0.06920162585527165\n",
      "Final MSE: 0.067760616816416\n",
      "Final MSE: 0.06808678587783995\n",
      "Final MSE: 0.06812347636454014\n",
      "Final MSE: 0.06700494574861329\n",
      "Final MSE: 0.0654125852278202\n",
      "Final MSE: 0.06639043220729243\n",
      "Final MSE: 0.06654414097780918\n",
      "Final MSE: 0.07585359308065014\n",
      "Final MSE: 1.000886914967035\n",
      "Final MSE: 0.0759249009772424\n",
      "Final MSE: 0.0644426250721964\n",
      "Final MSE: 0.0644714896374448\n",
      "Final MSE: 0.06472840270709364\n",
      "Final MSE: 0.06469111909850084\n",
      "Final MSE: 0.06090429225750839\n",
      "Final MSE: 0.06739046099032969\n",
      "Final MSE: 0.0652973106814003\n",
      "Final MSE: 0.06358963796309802\n",
      "Final MSE: 0.041237042078457374\n",
      "Final MSE: 0.0647499565214179\n",
      "Final MSE: 0.06540214266921388\n",
      "Final MSE: 0.06332050412625527\n",
      "Final MSE: 0.06523831431577559\n",
      "Final MSE: 0.06127484481736538\n",
      "Final MSE: 1.000010265203191\n",
      "Final MSE: 0.06445355538278255\n",
      "Final MSE: 0.05173135326938019\n",
      "Final MSE: 0.06751312727962486\n",
      "Final MSE: 0.041842593712804625\n",
      "Final MSE: 0.0631914851803709\n",
      "Final MSE: 0.05117648733175157\n",
      "Final MSE: 0.06565934798021654\n",
      "Final MSE: 0.04295582895542484\n",
      "Final MSE: 0.026114252430507794\n",
      "Final MSE: 0.020328876896600952\n",
      "Final MSE: 0.05134092394092047\n",
      "Final MSE: 0.03338740522846543\n",
      "Final MSE: 1.0000337947444358\n",
      "Final MSE: 0.047192274670782595\n",
      "Final MSE: 0.06201085725782232\n",
      "Final MSE: 0.06568069332887233\n",
      "Final MSE: 0.0650209530312263\n",
      "Final MSE: 0.06097486176611881\n",
      "Final MSE: 0.046119454882726146\n",
      "Final MSE: 0.05220579279371366\n",
      "Final MSE: 0.020327174223300194\n",
      "Final MSE: 0.05708953644563336\n",
      "Final MSE: 0.005504936478750431\n",
      "Final MSE: 0.009043025502659712\n",
      "Final MSE: 0.009453176398307\n",
      "Final MSE: 0.03789490549332768\n",
      "Final MSE: 0.01845852354878825\n",
      "Final MSE: 0.040291146296983486\n",
      "Final MSE: 0.06508287897724704\n",
      "Final MSE: 0.06343907509368843\n",
      "Final MSE: 0.06343068597312176\n",
      "Final MSE: 0.028063218169193837\n",
      "Final MSE: 0.034080844202114546\n",
      "Final MSE: 0.009589481432551745\n",
      "Final MSE: 0.01344462277185844\n",
      "Final MSE: 0.01162972214618202\n",
      "Final MSE: 0.026006539317751237\n",
      "Final MSE: 0.013748938856542806\n",
      "Final MSE: 0.11776440962985324\n",
      "Final MSE: 0.008644787892678639\n",
      "Final MSE: 0.00875332517412888\n",
      "Final MSE: 0.010659771656270075\n",
      "Final MSE: 0.06986746247442857\n",
      "Final MSE: 0.06859140626370691\n",
      "Final MSE: 0.06982283777673068\n",
      "Final MSE: 0.06845594936295415\n",
      "Final MSE: 0.06809499280808344\n",
      "Final MSE: 0.066072202784166\n",
      "Final MSE: 0.06503310046927298\n",
      "Final MSE: 0.07075509347228359\n",
      "Final MSE: 0.06855981296856484\n",
      "Final MSE: 0.06759361247917181\n",
      "Final MSE: 0.0370510059464769\n",
      "Final MSE: 0.06508978627410981\n",
      "Final MSE: 0.04884113952271441\n",
      "Final MSE: 0.06530367469797027\n",
      "Final MSE: 0.06697578681505256\n",
      "Final MSE: 0.03997838592755319\n",
      "Final MSE: 0.0646738863685549\n",
      "Final MSE: 0.04032385208979783\n",
      "Final MSE: 0.0381982090074416\n",
      "Final MSE: 0.0701209665112787\n",
      "Final MSE: 0.03420372665506695\n",
      "Final MSE: 0.037088466367091975\n",
      "Final MSE: 0.029231997541462027\n",
      "Final MSE: 0.028388136411491367\n",
      "Final MSE: 0.03425619050811372\n",
      "Final MSE: 0.02176943870810933\n",
      "Final MSE: 0.025184285091106078\n",
      "Final MSE: 0.022038055614144823\n",
      "Final MSE: 0.021629433649774207\n",
      "Final MSE: 0.02051714113669745\n",
      "Final MSE: 0.06521942723071013\n",
      "Final MSE: 0.06473883842072849\n",
      "Final MSE: 0.05337954702061477\n",
      "Final MSE: 0.07393850769219579\n",
      "Final MSE: 0.034805600579201956\n",
      "Final MSE: 0.01556285944273683\n",
      "Final MSE: 0.019256347622034237\n",
      "Final MSE: 0.017633130739900135\n",
      "Final MSE: 0.01806236352119233\n",
      "Final MSE: 0.017430848609491033\n",
      "Final MSE: 0.011152283482454905\n",
      "Final MSE: 0.010088029000373734\n",
      "Final MSE: 0.015718219285611516\n",
      "Final MSE: 0.008906437506245347\n",
      "Final MSE: 0.010687153775462761\n",
      "Final MSE: 0.035449123515777364\n",
      "Final MSE: 0.04171263729103007\n",
      "Final MSE: 0.032556516940531216\n",
      "Final MSE: 0.06640855471995488\n",
      "Final MSE: 0.06515420651302264\n",
      "Final MSE: 0.015922844690611424\n",
      "Final MSE: 0.021293308391921527\n",
      "Final MSE: 0.02005321206151677\n",
      "Final MSE: 0.012254717637278487\n",
      "Final MSE: 0.012296582760769201\n",
      "Final MSE: 0.007889322274987171\n",
      "Final MSE: 0.00607504165685736\n",
      "Final MSE: 0.007827485789581357\n",
      "Final MSE: 0.00716772185719939\n",
      "Final MSE: 0.009775751577607858\n",
      "Final MSE: 0.07607764011140357\n",
      "Final MSE: 0.07610766045834146\n",
      "Final MSE: 0.07601263628656742\n",
      "Final MSE: 0.07587712179409664\n",
      "Final MSE: 0.07586694386983649\n",
      "Final MSE: 0.07709779167295097\n",
      "Final MSE: 0.0783024357528448\n",
      "Final MSE: 0.07648352345270266\n",
      "Final MSE: 0.07672775529809729\n",
      "Final MSE: 0.07654147931964095\n",
      "Final MSE: 0.07627125203765274\n",
      "Final MSE: 0.07651230705590131\n",
      "Final MSE: 0.07967926839122044\n",
      "Final MSE: 0.07604995316625482\n",
      "Final MSE: 0.07633175305833381\n",
      "Final MSE: 0.07683746271519795\n",
      "Final MSE: 0.07594662629448698\n",
      "Final MSE: 0.07590096233739103\n",
      "Final MSE: 0.07586250013195167\n",
      "Final MSE: 0.07593399685216619\n",
      "Final MSE: 0.07635422118838855\n",
      "Final MSE: 0.07592365179635296\n",
      "Final MSE: 0.07815856652856347\n",
      "Final MSE: 0.07590715559323051\n",
      "Final MSE: 0.07614685258413693\n",
      "Final MSE: 0.0760557708262424\n",
      "Final MSE: 0.07595947825843165\n",
      "Final MSE: 0.07601300797829863\n",
      "Final MSE: 0.07604386201228044\n",
      "Final MSE: 0.07659280786399475\n",
      "Final MSE: 0.07591624739528918\n",
      "Final MSE: 0.07620203348303167\n",
      "Final MSE: 0.07675530904109373\n",
      "Final MSE: 0.07655274173448731\n",
      "Final MSE: 0.07592784724890451\n",
      "Final MSE: 0.07649135029599934\n",
      "Final MSE: 0.07658522870931078\n",
      "Final MSE: 0.07659210652655252\n",
      "Final MSE: 0.07590446852860384\n",
      "Final MSE: 0.076169613808014\n",
      "Final MSE: 0.07585707827247373\n",
      "Final MSE: 0.07604001133658503\n",
      "Final MSE: 0.07643837708763376\n",
      "Final MSE: 0.07607274921559513\n",
      "Final MSE: 0.0759717959121211\n",
      "Final MSE: 0.07887501354898\n",
      "Final MSE: 0.07931023225878155\n",
      "Final MSE: 0.08046189902878269\n",
      "Final MSE: 0.07692259977398631\n",
      "Final MSE: 0.08069048756443956\n",
      "Final MSE: 0.0759259620186138\n",
      "Final MSE: 0.07733900643331834\n",
      "Final MSE: 0.07682041009080753\n",
      "Final MSE: 0.07606774433327054\n",
      "Final MSE: 0.0761809423643439\n",
      "Final MSE: 0.07608070529318418\n",
      "Final MSE: 0.07585561190378576\n",
      "Final MSE: 0.07667595933036934\n",
      "Final MSE: 0.07601157367219827\n",
      "Final MSE: 0.07610936042806321\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:11:11.881087Z",
     "start_time": "2025-04-08T10:11:11.869936Z"
    }
   },
   "cell_type": "code",
   "source": "results_steps_large",
   "id": "fb960d6ba1a67dd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            1-2-1     1-4-1     1-8-1   1-2-2-1   1-4-4-1   1-8-8-1 1-2-2-2-1  \\\n",
       "sigmoid  0.070622  0.070419   0.07024  0.071487  0.070531  0.070506  0.071855   \n",
       "relu     0.256316  0.064602   0.05966  0.248542  0.057877  0.034825  0.247988   \n",
       "tanh     0.068967  0.067603  0.056652  0.050659  0.032634  0.022228  0.058416   \n",
       "linear   0.075988  0.077031  0.076969  0.076096  0.076498  0.076133  0.076271   \n",
       "\n",
       "        1-4-4-4-1 1-8-8-8-1    1-16-1 1-16-16-1 1-16-16-16-1  \n",
       "sigmoid  0.071066  0.069351  0.070489  0.067454     0.066695  \n",
       "relu     0.047343  0.016071  0.052061   0.01895     0.031914  \n",
       "tanh     0.017589   0.01131  0.048256  0.016364     0.007747  \n",
       "linear   0.076349  0.076076  0.079252  0.076467     0.076147  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-2-1</th>\n",
       "      <th>1-4-1</th>\n",
       "      <th>1-8-1</th>\n",
       "      <th>1-2-2-1</th>\n",
       "      <th>1-4-4-1</th>\n",
       "      <th>1-8-8-1</th>\n",
       "      <th>1-2-2-2-1</th>\n",
       "      <th>1-4-4-4-1</th>\n",
       "      <th>1-8-8-8-1</th>\n",
       "      <th>1-16-1</th>\n",
       "      <th>1-16-16-1</th>\n",
       "      <th>1-16-16-16-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigmoid</th>\n",
       "      <td>0.070622</td>\n",
       "      <td>0.070419</td>\n",
       "      <td>0.07024</td>\n",
       "      <td>0.071487</td>\n",
       "      <td>0.070531</td>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.071855</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.069351</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.067454</td>\n",
       "      <td>0.066695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>0.256316</td>\n",
       "      <td>0.064602</td>\n",
       "      <td>0.05966</td>\n",
       "      <td>0.248542</td>\n",
       "      <td>0.057877</td>\n",
       "      <td>0.034825</td>\n",
       "      <td>0.247988</td>\n",
       "      <td>0.047343</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.052061</td>\n",
       "      <td>0.01895</td>\n",
       "      <td>0.031914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>0.068967</td>\n",
       "      <td>0.067603</td>\n",
       "      <td>0.056652</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.032634</td>\n",
       "      <td>0.022228</td>\n",
       "      <td>0.058416</td>\n",
       "      <td>0.017589</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.016364</td>\n",
       "      <td>0.007747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.075988</td>\n",
       "      <td>0.077031</td>\n",
       "      <td>0.076969</td>\n",
       "      <td>0.076096</td>\n",
       "      <td>0.076498</td>\n",
       "      <td>0.076133</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.076349</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.079252</td>\n",
       "      <td>0.076467</td>\n",
       "      <td>0.076147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wygrywa tanh 1-16-16-16-1 a z relu wygrywa 1-8-8-8-1",
   "id": "aeaad12229cb4a20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# testy dla rings3",
   "id": "c5f6884160b432ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:14:36.554246Z",
     "start_time": "2025-04-08T10:14:02.820122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs_rings3 = {\n",
    "    \"train\": rings3_train,\n",
    "    \"test\": rings3_test\n",
    "}\n",
    "\n",
    "results_rings3 = train_model_gridsearch(\n",
    "    dfs=dfs_rings3,\n",
    "    architectures = {\n",
    "        \"1-2-1\": [2, 2, 3],\n",
    "        \"1-4-1\": [2, 4, 3],\n",
    "        \"1-8-1\": [2, 8, 3],\n",
    "        \"1-2-2-1\": [2, 2, 2, 3],\n",
    "        \"1-4-4-1\": [2, 4, 4, 3],\n",
    "        \"1-8-8-1\": [2, 8, 8, 3],\n",
    "        \"1-2-2-2-1\": [2, 2, 2, 2, 3],\n",
    "        \"1-4-4-4-1\": [2, 4, 4, 4, 3],\n",
    "        \"1-8-8-8-1\": [2, 8, 8, 8, 3],\n",
    "        \"1-16-1\": [2, 16, 3],\n",
    "        \"1-16-16-1\": [2, 16, 16, 3],\n",
    "        \"1-16-16-16-1\": [2, 16, 16, 16, 3]\n",
    "    },\n",
    "    activations=[\"sigmoid\", \"relu\", \"tanh\", \"linear\"],\n",
    "    classification=True,\n",
    "    output_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "6160354eecd714ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 406.09206869109903\n",
      "Final loss: 407.37963793088903\n",
      "Final loss: 474.25656053100874\n",
      "Final loss: 442.1089218392779\n",
      "Final loss: 440.63290528733677\n",
      "Final loss: 274.59638666096686\n",
      "Final loss: 252.00631348013675\n",
      "Final loss: 308.84281831150247\n",
      "Final loss: 285.8755737453204\n",
      "Final loss: 251.35206580002114\n",
      "Final loss: 186.69372949456397\n",
      "Final loss: 207.09257676726054\n",
      "Final loss: 207.66984588509823\n",
      "Final loss: 215.42715056897728\n",
      "Final loss: 257.90821552819506\n",
      "Final loss: 441.88026647700855\n",
      "Final loss: 453.6254780919026\n",
      "Final loss: 347.2187729106558\n",
      "Final loss: 345.6536668805373\n",
      "Final loss: 456.31154846750434\n",
      "Final loss: 276.1384507484757\n",
      "Final loss: 217.39366543616245\n",
      "Final loss: 261.1835862922001\n",
      "Final loss: 233.80000150864225\n",
      "Final loss: 224.2675796281459\n",
      "Final loss: 159.85611645727465\n",
      "Final loss: 160.6562954827267\n",
      "Final loss: 154.49164487906867\n",
      "Final loss: 160.96366411384432\n",
      "Final loss: 150.38744871611416\n",
      "Final loss: 346.3850477985302\n",
      "Final loss: 339.29613599911295\n",
      "Final loss: 340.3544584002212\n",
      "Final loss: 347.64911003203156\n",
      "Final loss: 457.24152674529137\n",
      "Final loss: 164.03789623982183\n",
      "Final loss: 456.14304770306927\n",
      "Final loss: 137.6793436482853\n",
      "Final loss: 195.58540094050718\n",
      "Final loss: 452.68411538404234\n",
      "Final loss: 120.49077517714123\n",
      "Final loss: 188.7113609111515\n",
      "Final loss: 173.493523945824\n",
      "Final loss: 148.26028262134267\n",
      "Final loss: 131.22917045357138\n",
      "Final loss: 206.83073218273228\n",
      "Final loss: 165.8453596447955\n",
      "Final loss: 211.35557909411878\n",
      "Final loss: 199.46777619154264\n",
      "Final loss: 202.25379483182974\n",
      "Final loss: 109.1451587420806\n",
      "Final loss: 139.56951460413617\n",
      "Final loss: 156.67502741797566\n",
      "Final loss: 134.47632512531945\n",
      "Final loss: 139.04236402222685\n",
      "Final loss: 126.07782152545133\n",
      "Final loss: 112.98741853367734\n",
      "Final loss: 136.17654979202655\n",
      "Final loss: 109.77021348558445\n",
      "Final loss: 126.51262904857015\n",
      "Final loss: 470.9905089611155\n",
      "Final loss: 442.5385659141364\n",
      "Final loss: 467.448701216914\n",
      "Final loss: 497.5052748105654\n",
      "Final loss: 430.7145311073039\n",
      "Final loss: 442.9476212458906\n",
      "Final loss: 246.72724445370565\n",
      "Final loss: 260.3371735307614\n",
      "Final loss: 263.1893871983291\n",
      "Final loss: 333.0459069785154\n",
      "Final loss: 111.88601225435025\n",
      "Final loss: 127.04890504973719\n",
      "Final loss: 137.73658055496045\n",
      "Final loss: 140.4996820330408\n",
      "Final loss: 128.82730257527663\n",
      "Final loss: 476.93967991098776\n",
      "Final loss: 467.9224009328231\n",
      "Final loss: 549.5108695400723\n",
      "Final loss: 470.7417569717857\n",
      "Final loss: 449.906621292743\n",
      "Final loss: 287.76896588239725\n",
      "Final loss: 176.0263085889918\n",
      "Final loss: 313.61612096652306\n",
      "Final loss: 427.15094265427246\n",
      "Final loss: 268.2486914637602\n",
      "Final loss: 77.35386779859205\n",
      "Final loss: 107.3103985750102\n",
      "Final loss: 122.28026549636995\n",
      "Final loss: 80.21192932346594\n",
      "Final loss: 97.23194220640647\n",
      "Final loss: 549.4591313280847\n",
      "Final loss: 549.411196535544\n",
      "Final loss: 480.1848734028085\n",
      "Final loss: 428.7861641416805\n",
      "Final loss: 371.66107515655676\n",
      "Final loss: 260.01740554690645\n",
      "Final loss: 436.4285075000651\n",
      "Final loss: 171.1874806095334\n",
      "Final loss: 193.8052622076622\n",
      "Final loss: 312.6454191631465\n",
      "Final loss: 97.67071518676941\n",
      "Final loss: 95.86417360050041\n",
      "Final loss: 93.67020822204337\n",
      "Final loss: 61.473871823520575\n",
      "Final loss: 94.07123923691366\n",
      "Final loss: 114.95834051088472\n",
      "Final loss: 114.96982632662211\n",
      "Final loss: 121.44287443277176\n",
      "Final loss: 112.0726742423838\n",
      "Final loss: 138.46658690715537\n",
      "Final loss: 90.87272661826735\n",
      "Final loss: 75.43177195140048\n",
      "Final loss: 74.6824074308834\n",
      "Final loss: 58.899327219841574\n",
      "Final loss: 79.70254041440279\n",
      "Final loss: 127.25196755365057\n",
      "Final loss: 91.62091006741692\n",
      "Final loss: 79.10899915305434\n",
      "Final loss: 92.47678794843638\n",
      "Final loss: 58.345182393513674\n",
      "Final loss: 428.3503737422736\n",
      "Final loss: 397.67934091605275\n",
      "Final loss: 396.3412938489476\n",
      "Final loss: 412.5569994588728\n",
      "Final loss: 425.52887431956196\n",
      "Final loss: 287.71097482742056\n",
      "Final loss: 243.15074971593586\n",
      "Final loss: 255.88942659473014\n",
      "Final loss: 234.00918483590362\n",
      "Final loss: 239.24549868813403\n",
      "Final loss: 157.4416761799075\n",
      "Final loss: 150.72737834288773\n",
      "Final loss: 151.0149918679006\n",
      "Final loss: 157.80533617065143\n",
      "Final loss: 147.7318573781155\n",
      "Final loss: 324.82878971928494\n",
      "Final loss: 329.41541570930394\n",
      "Final loss: 399.25800966289444\n",
      "Final loss: 473.03419743199464\n",
      "Final loss: 375.6926721939179\n",
      "Final loss: 149.48568514077138\n",
      "Final loss: 126.77261582671953\n",
      "Final loss: 152.97751635439258\n",
      "Final loss: 124.00833637125345\n",
      "Final loss: 147.9207990054405\n",
      "Final loss: 119.72127030680224\n",
      "Final loss: 93.35803025297143\n",
      "Final loss: 92.70922394161748\n",
      "Final loss: 84.1302932994933\n",
      "Final loss: 81.50359700546225\n",
      "Final loss: 447.75006928588346\n",
      "Final loss: 339.27285922566165\n",
      "Final loss: 327.78779854588873\n",
      "Final loss: 382.4858413798843\n",
      "Final loss: 377.8543872887032\n",
      "Final loss: 183.46170898568963\n",
      "Final loss: 136.81110317631843\n",
      "Final loss: 131.5030733050493\n",
      "Final loss: 130.13590126858443\n",
      "Final loss: 180.95523540408908\n",
      "Final loss: 144.1375069169793\n",
      "Final loss: 72.22462618745833\n",
      "Final loss: 82.83317349075803\n",
      "Final loss: 102.0633295807739\n",
      "Final loss: 95.79633858249652\n",
      "Final loss: 113.45039603241685\n",
      "Final loss: 115.33707246860529\n",
      "Final loss: 130.44949260951213\n",
      "Final loss: 115.95073216517115\n",
      "Final loss: 129.56175020614913\n",
      "Final loss: 76.37746813549363\n",
      "Final loss: 60.39788740551558\n",
      "Final loss: 62.68259882649725\n",
      "Final loss: 95.39230972099415\n",
      "Final loss: 98.70515450833915\n",
      "Final loss: 61.919429565642076\n",
      "Final loss: 52.73668163008994\n",
      "Final loss: 56.41935751293793\n",
      "Final loss: 59.94943583988621\n",
      "Final loss: 82.47805862143554\n",
      "Final loss: 494.51623638510455\n",
      "Final loss: 495.3038045458367\n",
      "Final loss: 496.8035230343364\n",
      "Final loss: 495.16307040165174\n",
      "Final loss: 494.6230620786658\n",
      "Final loss: 496.7401541592453\n",
      "Final loss: 495.0715708616339\n",
      "Final loss: 498.0288511170682\n",
      "Final loss: 496.52859936344703\n",
      "Final loss: 495.3196067631383\n",
      "Final loss: 496.90617597296506\n",
      "Final loss: 500.5523256399154\n",
      "Final loss: 495.4425280099014\n",
      "Final loss: 496.1125952035213\n",
      "Final loss: 506.0385521378071\n",
      "Final loss: 496.585294940473\n",
      "Final loss: 497.04439703103145\n",
      "Final loss: 495.6293350478035\n",
      "Final loss: 497.8113950574229\n",
      "Final loss: 495.41217969788676\n",
      "Final loss: 499.39365995075906\n",
      "Final loss: 496.81189591322027\n",
      "Final loss: 496.08688107026137\n",
      "Final loss: 495.376160149297\n",
      "Final loss: 495.6855488640938\n",
      "Final loss: 495.60335113844957\n",
      "Final loss: 495.0125126637084\n",
      "Final loss: 497.566122501946\n",
      "Final loss: 496.1550501867503\n",
      "Final loss: 495.79374863210296\n",
      "Final loss: 497.1330398285602\n",
      "Final loss: 496.1362974779602\n",
      "Final loss: 495.7471167758877\n",
      "Final loss: 496.87119552452936\n",
      "Final loss: 500.9867343795582\n",
      "Final loss: 495.92986195065464\n",
      "Final loss: 496.7199062578649\n",
      "Final loss: 496.9976753103747\n",
      "Final loss: 497.22798985654464\n",
      "Final loss: 500.49157443013314\n",
      "Final loss: 495.6667093525966\n",
      "Final loss: 496.6314994820993\n",
      "Final loss: 496.14573398254964\n",
      "Final loss: 496.7322524453387\n",
      "Final loss: 495.31860146117407\n",
      "Final loss: 497.05288709625466\n",
      "Final loss: 505.151533902548\n",
      "Final loss: 510.6540089429298\n",
      "Final loss: 519.2060444690935\n",
      "Final loss: 503.0355503201634\n",
      "Final loss: 568.1795639363244\n",
      "Final loss: 525.8770555483744\n",
      "Final loss: 551.041713412335\n",
      "Final loss: 617.0150224567027\n",
      "Final loss: 557.712897918455\n",
      "Final loss: 495.6440959681272\n",
      "Final loss: 526.4898364892023\n",
      "Final loss: 547.1586475223445\n",
      "Final loss: 497.63985895572904\n",
      "Final loss: 876.4575825475755\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:16:45.046292Z",
     "start_time": "2025-04-08T10:16:45.035259Z"
    }
   },
   "cell_type": "code",
   "source": "results_rings3\n",
   "id": "e5eca7f46bac5311",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              1-2-1       1-4-1       1-8-1     1-2-2-1     1-4-4-1  \\\n",
       "sigmoid  434.094019  274.534632  214.958304  408.937947  242.556657   \n",
       "relu     461.839516  309.249467  129.199696  483.004266  294.562206   \n",
       "tanh     412.091376  252.001167  152.944248  380.445817  140.232991   \n",
       "linear   495.281939  496.337756  499.010435   496.49652  496.670829   \n",
       "\n",
       "            1-8-8-1   1-2-2-2-1   1-4-4-4-1   1-8-8-8-1      1-16-1  \\\n",
       "sigmoid  157.271034  366.185256  281.225961  152.437023  197.150648   \n",
       "relu      96.877681  475.900488  274.816815   88.550042   120.38206   \n",
       "tanh      94.284483  375.030191  152.573404   99.410995  120.949889   \n",
       "linear   496.026157  497.374877  497.473402  496.098959  507.020005   \n",
       "\n",
       "          1-16-16-1 1-16-16-16-1  \n",
       "sigmoid  135.781678   122.304926  \n",
       "relu      75.917755    89.760769  \n",
       "tanh      78.711084    62.700593  \n",
       "linear   563.965251   588.678004  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-2-1</th>\n",
       "      <th>1-4-1</th>\n",
       "      <th>1-8-1</th>\n",
       "      <th>1-2-2-1</th>\n",
       "      <th>1-4-4-1</th>\n",
       "      <th>1-8-8-1</th>\n",
       "      <th>1-2-2-2-1</th>\n",
       "      <th>1-4-4-4-1</th>\n",
       "      <th>1-8-8-8-1</th>\n",
       "      <th>1-16-1</th>\n",
       "      <th>1-16-16-1</th>\n",
       "      <th>1-16-16-16-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigmoid</th>\n",
       "      <td>434.094019</td>\n",
       "      <td>274.534632</td>\n",
       "      <td>214.958304</td>\n",
       "      <td>408.937947</td>\n",
       "      <td>242.556657</td>\n",
       "      <td>157.271034</td>\n",
       "      <td>366.185256</td>\n",
       "      <td>281.225961</td>\n",
       "      <td>152.437023</td>\n",
       "      <td>197.150648</td>\n",
       "      <td>135.781678</td>\n",
       "      <td>122.304926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>461.839516</td>\n",
       "      <td>309.249467</td>\n",
       "      <td>129.199696</td>\n",
       "      <td>483.004266</td>\n",
       "      <td>294.562206</td>\n",
       "      <td>96.877681</td>\n",
       "      <td>475.900488</td>\n",
       "      <td>274.816815</td>\n",
       "      <td>88.550042</td>\n",
       "      <td>120.38206</td>\n",
       "      <td>75.917755</td>\n",
       "      <td>89.760769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>412.091376</td>\n",
       "      <td>252.001167</td>\n",
       "      <td>152.944248</td>\n",
       "      <td>380.445817</td>\n",
       "      <td>140.232991</td>\n",
       "      <td>94.284483</td>\n",
       "      <td>375.030191</td>\n",
       "      <td>152.573404</td>\n",
       "      <td>99.410995</td>\n",
       "      <td>120.949889</td>\n",
       "      <td>78.711084</td>\n",
       "      <td>62.700593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>495.281939</td>\n",
       "      <td>496.337756</td>\n",
       "      <td>499.010435</td>\n",
       "      <td>496.49652</td>\n",
       "      <td>496.670829</td>\n",
       "      <td>496.026157</td>\n",
       "      <td>497.374877</td>\n",
       "      <td>497.473402</td>\n",
       "      <td>496.098959</td>\n",
       "      <td>507.020005</td>\n",
       "      <td>563.965251</td>\n",
       "      <td>588.678004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### wygrywa dla tanh 1-16-16-16-1 oraz relu 1-16-16-1",
   "id": "887f433b43e505ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# testy dla rings5",
   "id": "d52e3b1d6399332e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:17:45.210309Z",
     "start_time": "2025-04-08T10:17:15.243971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs_rings5 = {\n",
    "    \"train\": rings5_train,\n",
    "    \"test\": rings5_test\n",
    "}\n",
    "\n",
    "results_rings5 = train_model_gridsearch(\n",
    "    dfs=dfs_rings5,\n",
    "    architectures = {\n",
    "        \"1-2-1\": [2, 2, 5],\n",
    "        \"1-4-1\": [2, 4, 5],\n",
    "        \"1-8-1\": [2, 8, 5],\n",
    "        \"1-2-2-1\": [2, 2, 2, 5],\n",
    "        \"1-4-4-1\": [2, 4, 4, 5],\n",
    "        \"1-8-8-1\": [2, 8, 8, 5],\n",
    "        \"1-2-2-2-1\": [2, 2, 2, 2, 5],\n",
    "        \"1-4-4-4-1\": [2, 4, 4, 4, 5],\n",
    "        \"1-8-8-8-1\": [2, 8, 8, 8, 5],\n",
    "        \"1-16-1\": [2, 16, 5],\n",
    "        \"1-16-16-1\": [2, 16, 16, 5],\n",
    "        \"1-16-16-16-1\": [2, 16, 16, 16, 5]\n",
    "    },\n",
    "    activations=[\"sigmoid\", \"relu\", \"tanh\", \"linear\"],\n",
    "    classification=True,\n",
    "    output_activation=\"softmax\"\n",
    ")"
   ],
   "id": "3708549fdb5aafce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 315.7033505544106\n",
      "Final loss: 283.40613509083795\n",
      "Final loss: 315.27524308755955\n",
      "Final loss: 295.9780353648809\n",
      "Final loss: 315.7340461683324\n",
      "Final loss: 204.27724887617296\n",
      "Final loss: 226.37601562649235\n",
      "Final loss: 234.79117657064398\n",
      "Final loss: 265.39352208571364\n",
      "Final loss: 228.10369594109687\n",
      "Final loss: 167.52076172377284\n",
      "Final loss: 188.81624497039292\n",
      "Final loss: 193.2108039495632\n",
      "Final loss: 201.26922921702914\n",
      "Final loss: 189.5424248445243\n",
      "Final loss: 241.02109695786186\n",
      "Final loss: 236.60338455131463\n",
      "Final loss: 241.06174688363973\n",
      "Final loss: 243.90952982142176\n",
      "Final loss: 287.40533348331763\n",
      "Final loss: 138.81862706414202\n",
      "Final loss: 215.144287223376\n",
      "Final loss: 208.52202813524696\n",
      "Final loss: 153.68212838285976\n",
      "Final loss: 160.00271374672752\n",
      "Final loss: 131.70041187295556\n",
      "Final loss: 130.75757578420772\n",
      "Final loss: 144.2593254407688\n",
      "Final loss: 162.08449364414298\n",
      "Final loss: 116.13411874991466\n",
      "Final loss: 238.63175740650678\n",
      "Final loss: 224.01296242109456\n",
      "Final loss: 232.61845935971087\n",
      "Final loss: 225.874895261123\n",
      "Final loss: 247.37144602090484\n",
      "Final loss: 196.09441817182548\n",
      "Final loss: 142.1703292067525\n",
      "Final loss: 148.04106026177462\n",
      "Final loss: 148.475159488282\n",
      "Final loss: 164.64452421176043\n",
      "Final loss: 92.74115695473901\n",
      "Final loss: 84.21722146474481\n",
      "Final loss: 84.31745168012685\n",
      "Final loss: 90.72659110851461\n",
      "Final loss: 89.31908538563718\n",
      "Final loss: 157.9578635912357\n",
      "Final loss: 164.72300016075093\n",
      "Final loss: 152.19870622030254\n",
      "Final loss: 165.76417052908147\n",
      "Final loss: 156.94518308765203\n",
      "Final loss: 99.77640287697459\n",
      "Final loss: 115.68420993264586\n",
      "Final loss: 109.0255605220348\n",
      "Final loss: 98.25892402341762\n",
      "Final loss: 121.13164310541865\n",
      "Final loss: 90.16340346584037\n",
      "Final loss: 97.8025451829687\n",
      "Final loss: 92.24469643751063\n",
      "Final loss: 99.2679136035445\n",
      "Final loss: 95.39076282776952\n",
      "Final loss: 337.0218395676537\n",
      "Final loss: 319.9175879231201\n",
      "Final loss: 293.2382415801362\n",
      "Final loss: 319.18709741327655\n",
      "Final loss: 297.76327440283234\n",
      "Final loss: 147.4079566417059\n",
      "Final loss: 120.44909349655356\n",
      "Final loss: 130.12817905227692\n",
      "Final loss: 271.1671319291139\n",
      "Final loss: 139.93694391858176\n",
      "Final loss: 121.09860760350284\n",
      "Final loss: 114.58097303013328\n",
      "Final loss: 116.19231774262448\n",
      "Final loss: 126.62950463058783\n",
      "Final loss: 110.30808380928079\n",
      "Final loss: 294.66682744040907\n",
      "Final loss: 227.98335893412394\n",
      "Final loss: 322.85728407910375\n",
      "Final loss: 223.4690853639786\n",
      "Final loss: 402.380876817202\n",
      "Final loss: 176.87503860919244\n",
      "Final loss: 319.45968799876863\n",
      "Final loss: 113.81972416522713\n",
      "Final loss: 104.99795649409131\n",
      "Final loss: 132.82899798469748\n",
      "Final loss: 78.47560383951011\n",
      "Final loss: 89.76950634524054\n",
      "Final loss: 88.19114043285416\n",
      "Final loss: 69.6131215943474\n",
      "Final loss: 86.0060591013731\n",
      "Final loss: 402.44685811936426\n",
      "Final loss: 402.4633591548761\n",
      "Final loss: 402.40174379268285\n",
      "Final loss: 325.29985271851376\n",
      "Final loss: 348.4645345713336\n",
      "Final loss: 167.65916929337365\n",
      "Final loss: 159.56707229822592\n",
      "Final loss: 116.19324351883229\n",
      "Final loss: 247.83519066454772\n",
      "Final loss: 150.31254234267334\n",
      "Final loss: 84.9342457069494\n",
      "Final loss: 85.74705015198069\n",
      "Final loss: 91.27948705120276\n",
      "Final loss: 98.99574755260285\n",
      "Final loss: 86.34582413647487\n",
      "Final loss: 104.94466090040972\n",
      "Final loss: 121.92971913961692\n",
      "Final loss: 109.9456548245984\n",
      "Final loss: 95.2572606439679\n",
      "Final loss: 102.96791657710727\n",
      "Final loss: 65.87079657598751\n",
      "Final loss: 74.92773580980766\n",
      "Final loss: 59.262632161430226\n",
      "Final loss: 72.34778394093182\n",
      "Final loss: 52.034200453877716\n",
      "Final loss: 91.38857878285188\n",
      "Final loss: 61.73325586490447\n",
      "Final loss: 59.953887697849225\n",
      "Final loss: 51.564757236159096\n",
      "Final loss: 56.20155978622531\n",
      "Final loss: 279.4769055612318\n",
      "Final loss: 278.87575282066456\n",
      "Final loss: 279.7614737050854\n",
      "Final loss: 278.02300702570665\n",
      "Final loss: 278.67531421920404\n",
      "Final loss: 194.5000773601301\n",
      "Final loss: 217.81733761094205\n",
      "Final loss: 231.20147189356285\n",
      "Final loss: 182.60615339643664\n",
      "Final loss: 185.20322651065723\n",
      "Final loss: 129.110672824361\n",
      "Final loss: 136.755438912529\n",
      "Final loss: 122.93250180555522\n",
      "Final loss: 128.7656389703255\n",
      "Final loss: 124.56286754229932\n",
      "Final loss: 261.70907467555116\n",
      "Final loss: 247.12848141539106\n",
      "Final loss: 256.7210656862717\n",
      "Final loss: 216.81896573133434\n",
      "Final loss: 257.9437726995139\n",
      "Final loss: 115.66450673213072\n",
      "Final loss: 136.84778910122927\n",
      "Final loss: 132.74330047990924\n",
      "Final loss: 143.35464700058\n",
      "Final loss: 160.72034682505898\n",
      "Final loss: 68.76113871638495\n",
      "Final loss: 81.65914403612321\n",
      "Final loss: 80.18747054318978\n",
      "Final loss: 85.73183393619681\n",
      "Final loss: 71.82467325604571\n",
      "Final loss: 221.40772383947356\n",
      "Final loss: 217.83571677661274\n",
      "Final loss: 216.912492745694\n",
      "Final loss: 257.30425470363974\n",
      "Final loss: 195.32388559083998\n",
      "Final loss: 107.25785329881505\n",
      "Final loss: 114.29314482658019\n",
      "Final loss: 163.5212741285265\n",
      "Final loss: 95.03155404532292\n",
      "Final loss: 137.26255553285853\n",
      "Final loss: 95.47504803281736\n",
      "Final loss: 71.58045679911304\n",
      "Final loss: 78.91223635786453\n",
      "Final loss: 72.39147821441206\n",
      "Final loss: 82.01749318975371\n",
      "Final loss: 109.48372751583346\n",
      "Final loss: 89.04433759632448\n",
      "Final loss: 100.23693898324254\n",
      "Final loss: 99.72462893039668\n",
      "Final loss: 87.68458095970367\n",
      "Final loss: 51.2812616671619\n",
      "Final loss: 54.4535647934747\n",
      "Final loss: 55.69989753252285\n",
      "Final loss: 55.1524358503636\n",
      "Final loss: 58.212763470290476\n",
      "Final loss: 61.825555298642335\n",
      "Final loss: 49.719696936961725\n",
      "Final loss: 38.469025006229195\n",
      "Final loss: 44.63834716329349\n",
      "Final loss: 43.85763517931214\n",
      "Final loss: 343.9172745460204\n",
      "Final loss: 343.1665434777361\n",
      "Final loss: 342.4298976566362\n",
      "Final loss: 343.1471684473048\n",
      "Final loss: 343.5721946264608\n",
      "Final loss: 343.0925821598429\n",
      "Final loss: 345.0998757902016\n",
      "Final loss: 344.9613199406284\n",
      "Final loss: 343.078161556757\n",
      "Final loss: 343.31264640969187\n",
      "Final loss: 343.2015296082352\n",
      "Final loss: 344.48047050639383\n",
      "Final loss: 344.0556055357062\n",
      "Final loss: 344.505541598965\n",
      "Final loss: 344.35663438250697\n",
      "Final loss: 343.36394083261183\n",
      "Final loss: 342.7638278818532\n",
      "Final loss: 346.0220936919606\n",
      "Final loss: 349.093800452814\n",
      "Final loss: 344.06447717889813\n",
      "Final loss: 343.2302244777631\n",
      "Final loss: 344.2007708065892\n",
      "Final loss: 347.2986300071615\n",
      "Final loss: 343.2021196541092\n",
      "Final loss: 343.1686358959352\n",
      "Final loss: 352.20165918041437\n",
      "Final loss: 347.727905868163\n",
      "Final loss: 346.66918307998975\n",
      "Final loss: 350.26117022835456\n",
      "Final loss: 345.90101581302554\n",
      "Final loss: 343.69642090410093\n",
      "Final loss: 344.1691442497867\n",
      "Final loss: 347.6773697724802\n",
      "Final loss: 343.2757243902336\n",
      "Final loss: 343.00266820781115\n",
      "Final loss: 344.9821570752844\n",
      "Final loss: 343.1404876600283\n",
      "Final loss: 348.49577584971064\n",
      "Final loss: 342.8996278108982\n",
      "Final loss: 344.7403440825555\n",
      "Final loss: 343.39042016487\n",
      "Final loss: 346.16658529802135\n",
      "Final loss: 347.0257731550904\n",
      "Final loss: 352.35340758069015\n",
      "Final loss: 353.5146037265202\n",
      "Final loss: 346.8044335913287\n",
      "Final loss: 345.7478187750659\n",
      "Final loss: 344.4564277391652\n",
      "Final loss: 343.8578503366216\n",
      "Final loss: 344.7714191363895\n",
      "Final loss: 405.91134086932846\n",
      "Final loss: 365.916158294063\n",
      "Final loss: 374.6084955853895\n",
      "Final loss: 363.0390374722698\n",
      "Final loss: 384.6972299505353\n",
      "Final loss: 490.81194615634956\n",
      "Final loss: 516.397163579695\n",
      "Final loss: 533.9341442772633\n",
      "Final loss: 922.1844358674273\n",
      "Final loss: 673.0346109727659\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:17:49.816665Z",
     "start_time": "2025-04-08T10:17:49.807625Z"
    }
   },
   "cell_type": "code",
   "source": "results_rings5",
   "id": "f311c114cd8ae33d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              1-2-1       1-4-1       1-8-1     1-2-2-1     1-4-4-1  \\\n",
       "sigmoid  305.219362  231.788332  188.071893  250.000218  175.233957   \n",
       "relu     313.425608  161.817861  117.761897  294.271487  169.596281   \n",
       "tanh     278.962491  202.265653  128.425424  248.064272  137.866118   \n",
       "linear   343.246616  343.908917  344.119956  345.061628  344.220076   \n",
       "\n",
       "            1-8-8-1   1-2-2-2-1   1-4-4-4-1   1-8-8-8-1      1-16-1  \\\n",
       "sigmoid  136.987185  233.701904  159.885098   88.264301  159.517785   \n",
       "relu      82.411086   376.21527  168.313444   89.460471  107.009042   \n",
       "tanh      77.632852  221.756815  123.473276   80.075343   97.234843   \n",
       "linear   348.552187  344.364266  344.851678  348.490158   345.12759   \n",
       "\n",
       "          1-16-16-1 1-16-16-16-1  \n",
       "sigmoid  108.775348    94.973864  \n",
       "relu       64.88863    64.168408  \n",
       "tanh      54.959985    47.702052  \n",
       "linear   378.834452    627.27246  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1-2-1</th>\n",
       "      <th>1-4-1</th>\n",
       "      <th>1-8-1</th>\n",
       "      <th>1-2-2-1</th>\n",
       "      <th>1-4-4-1</th>\n",
       "      <th>1-8-8-1</th>\n",
       "      <th>1-2-2-2-1</th>\n",
       "      <th>1-4-4-4-1</th>\n",
       "      <th>1-8-8-8-1</th>\n",
       "      <th>1-16-1</th>\n",
       "      <th>1-16-16-1</th>\n",
       "      <th>1-16-16-16-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sigmoid</th>\n",
       "      <td>305.219362</td>\n",
       "      <td>231.788332</td>\n",
       "      <td>188.071893</td>\n",
       "      <td>250.000218</td>\n",
       "      <td>175.233957</td>\n",
       "      <td>136.987185</td>\n",
       "      <td>233.701904</td>\n",
       "      <td>159.885098</td>\n",
       "      <td>88.264301</td>\n",
       "      <td>159.517785</td>\n",
       "      <td>108.775348</td>\n",
       "      <td>94.973864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relu</th>\n",
       "      <td>313.425608</td>\n",
       "      <td>161.817861</td>\n",
       "      <td>117.761897</td>\n",
       "      <td>294.271487</td>\n",
       "      <td>169.596281</td>\n",
       "      <td>82.411086</td>\n",
       "      <td>376.21527</td>\n",
       "      <td>168.313444</td>\n",
       "      <td>89.460471</td>\n",
       "      <td>107.009042</td>\n",
       "      <td>64.88863</td>\n",
       "      <td>64.168408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tanh</th>\n",
       "      <td>278.962491</td>\n",
       "      <td>202.265653</td>\n",
       "      <td>128.425424</td>\n",
       "      <td>248.064272</td>\n",
       "      <td>137.866118</td>\n",
       "      <td>77.632852</td>\n",
       "      <td>221.756815</td>\n",
       "      <td>123.473276</td>\n",
       "      <td>80.075343</td>\n",
       "      <td>97.234843</td>\n",
       "      <td>54.959985</td>\n",
       "      <td>47.702052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>343.246616</td>\n",
       "      <td>343.908917</td>\n",
       "      <td>344.119956</td>\n",
       "      <td>345.061628</td>\n",
       "      <td>344.220076</td>\n",
       "      <td>348.552187</td>\n",
       "      <td>344.364266</td>\n",
       "      <td>344.851678</td>\n",
       "      <td>348.490158</td>\n",
       "      <td>345.12759</td>\n",
       "      <td>378.834452</td>\n",
       "      <td>627.27246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### wygrywa 1-16-16-16-1 relu oraz 1-16-16-16-1 oraz tanh",
   "id": "a4eca9de8a726804"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## steps_large",
   "id": "6f3e855c985a6f8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### relu, tutaj nie wybieram tego kto wygral, bo wygralo 1-8-8-8-1, ale dopiero ta architektura pozwolila mi zejsc do mse 3, ktory byl wymagany z 2 tygodnie temu",
   "id": "2d398242e59b4879"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:28:15.820689Z",
     "start_time": "2025-04-08T10:28:15.816591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)"
   ],
   "id": "944073886a1170d2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:30:27.501446Z",
     "start_time": "2025-04-08T10:28:16.375963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_0_v0 = NeuralNetwork5(layer_sizes=[1,16,16,16,1],\n",
    "                                  activation='relu',\n",
    "                                  output_activation='linear',\n",
    "                                  init_method='he',\n",
    "                                  classification = False)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_0_v0, y_train_scaled_0_v0,  x_test_scaled_0_v0, y_test_0_v0, scaling_params_0_v0 =  load_and_scale_data_regression(steps_large_train,\n",
    "                                                                                                        steps_large_test, 'x','y')\n",
    "\n",
    "neural_network_0_v0.train(x_train_scaled_0_v0, y_train_scaled_0_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v0.train(x_train_scaled_0_v0, y_train_scaled_0_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v0.train(x_train_scaled_0_v0, y_train_scaled_0_v0, batch_size = 200 ,epochs = 5000, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v0.train(x_train_scaled_0_v0, y_train_scaled_0_v0, batch_size = 200 ,epochs = 10000, learning_rate=0.005, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "ba917159ff6068df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, MSE: 0.006950468386940014\n",
      "Epoch 1000/2500, MSE: 0.005500858783549404\n",
      "Epoch 1500/2500, MSE: 0.002480614805124578\n",
      "Epoch 2000/2500, MSE: 0.0020219813453761716\n",
      "Epoch 2500/2500, MSE: 0.0026900614851047507\n",
      "Final MSE: 0.0026900614851047507\n",
      "Epoch 500/2500, MSE: 0.0014969014620098819\n",
      "Epoch 1000/2500, MSE: 0.0013382412632331985\n",
      "Epoch 1500/2500, MSE: 0.0012503443972652034\n",
      "Epoch 2000/2500, MSE: 0.0012348813244576183\n",
      "Epoch 2500/2500, MSE: 0.0009551882185865726\n",
      "Final MSE: 0.0009551882185865726\n",
      "Epoch 500/5000, MSE: 0.0011343751239334253\n",
      "Epoch 1000/5000, MSE: 0.0009882853046820049\n",
      "Epoch 1500/5000, MSE: 0.0008540782481838378\n",
      "Epoch 2000/5000, MSE: 0.0008693578016460321\n",
      "Epoch 2500/5000, MSE: 0.0008105161713140659\n",
      "Epoch 3000/5000, MSE: 0.0008789257132084857\n",
      "Epoch 3500/5000, MSE: 0.00076255118916802\n",
      "Epoch 4000/5000, MSE: 0.0008155291669283274\n",
      "Epoch 4500/5000, MSE: 0.0007317409019276834\n",
      "Epoch 5000/5000, MSE: 0.0007293142864070633\n",
      "Final MSE: 0.0007293142864070633\n",
      "Epoch 500/10000, MSE: 0.000693181145113747\n",
      "Epoch 1000/10000, MSE: 0.0006876077434960239\n",
      "Epoch 1500/10000, MSE: 0.0006749915055183325\n",
      "Epoch 2000/10000, MSE: 0.0006915975221299202\n",
      "Epoch 2500/10000, MSE: 0.000666286693783521\n",
      "Epoch 3000/10000, MSE: 0.000671326518984275\n",
      "Epoch 3500/10000, MSE: 0.00065168651246095\n",
      "Epoch 4000/10000, MSE: 0.0006835288851628308\n",
      "Epoch 4500/10000, MSE: 0.0007054785071380903\n",
      "Epoch 5000/10000, MSE: 0.0006526418268691621\n",
      "Epoch 5500/10000, MSE: 0.0006998841499090445\n",
      "Epoch 6000/10000, MSE: 0.0006433003500784725\n",
      "Epoch 6500/10000, MSE: 0.0006427688951997161\n",
      "Epoch 7000/10000, MSE: 0.0006751060341012047\n",
      "Epoch 7500/10000, MSE: 0.0006477495665711402\n",
      "Epoch 8000/10000, MSE: 0.0006209506253596358\n",
      "Epoch 8500/10000, MSE: 0.0005929510023745243\n",
      "Epoch 9000/10000, MSE: 0.0005883543837169433\n",
      "Epoch 9500/10000, MSE: 0.0006455014946211195\n",
      "Epoch 10000/10000, MSE: 0.0005894091608278995\n",
      "Final MSE: 0.0005894091608278995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0005894091608278995)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:30:38.097533Z",
     "start_time": "2025-04-08T10:30:38.082821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_predict_0_v0  = neural_network_0_v0.forward(x_test_scaled_0_v0)\n",
    "y_test_mean_0_v0 = scaling_params_0_v0[2]\n",
    "y_test_sd_0_v0 = scaling_params_0_v0[3]\n",
    "y_predict_rescaled_0_v0 = y_predict_0_v0 * y_test_sd_0_v0 + y_test_mean_0_v0\n",
    "neural_network_0_v0.compute_mse(y_test_0_v0, y_predict_rescaled_0_v0)"
   ],
   "id": "47557eb09e5c098b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.010448105699494)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### tanh",
   "id": "771dc7f51d464a3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:26.123129Z",
     "start_time": "2025-04-08T10:30:41.322687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_0_v1 = NeuralNetwork5(layer_sizes=[1,16,16,16,1],\n",
    "                                     activation='tanh',\n",
    "                                     output_activation='linear',\n",
    "                                     init_method='xavier',\n",
    "                                     classification = False)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_0_v1, y_train_scaled_0_v1,  x_test_scaled_0_v1, y_test_0_v1, scaling_params_0_v1 =  load_and_scale_data_regression(steps_large_train,\n",
    "                                                                                                                                  steps_large_test, 'x','y')\n",
    "\n",
    "neural_network_0_v1.train(x_train_scaled_0_v1, y_train_scaled_0_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v1.train(x_train_scaled_0_v1, y_train_scaled_0_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v1.train(x_train_scaled_0_v1, y_train_scaled_0_v1, batch_size = 200 ,epochs = 5000, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n",
    "neural_network_0_v1.train(x_train_scaled_0_v1, y_train_scaled_0_v1, batch_size = 200 ,epochs = 10000, learning_rate=0.005, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "d7fab6f0dc393447",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, MSE: 0.06975479423087147\n",
      "Epoch 1000/2500, MSE: 0.010678611074460474\n",
      "Epoch 1500/2500, MSE: 0.005448168668574284\n",
      "Epoch 2000/2500, MSE: 0.0036911901725424183\n",
      "Epoch 2500/2500, MSE: 0.003024579325018334\n",
      "Final MSE: 0.003024579325018334\n",
      "Epoch 500/2500, MSE: 0.002779356175888831\n",
      "Epoch 1000/2500, MSE: 0.002606385933725048\n",
      "Epoch 1500/2500, MSE: 0.0024924211136914884\n",
      "Epoch 2000/2500, MSE: 0.002317082948803508\n",
      "Epoch 2500/2500, MSE: 0.0022116648952616403\n",
      "Final MSE: 0.0022116648952616403\n",
      "Epoch 500/5000, MSE: 0.0021378966022000613\n",
      "Epoch 1000/5000, MSE: 0.0021133396550394768\n",
      "Epoch 1500/5000, MSE: 0.0020621651726712236\n",
      "Epoch 2000/5000, MSE: 0.0020300932127661894\n",
      "Epoch 2500/5000, MSE: 0.0019790014126508867\n",
      "Epoch 3000/5000, MSE: 0.0019434825334516334\n",
      "Epoch 3500/5000, MSE: 0.001919893501669537\n",
      "Epoch 4000/5000, MSE: 0.001876639062196283\n",
      "Epoch 4500/5000, MSE: 0.0018435091088224366\n",
      "Epoch 5000/5000, MSE: 0.0018282172081213216\n",
      "Final MSE: 0.0018282172081213216\n",
      "Epoch 500/10000, MSE: 0.0017980436124914573\n",
      "Epoch 1000/10000, MSE: 0.0017784869729413843\n",
      "Epoch 1500/10000, MSE: 0.0017617641099755233\n",
      "Epoch 2000/10000, MSE: 0.001746937192100699\n",
      "Epoch 2500/10000, MSE: 0.0017380432072030836\n",
      "Epoch 3000/10000, MSE: 0.0017225133525892642\n",
      "Epoch 3500/10000, MSE: 0.0017038114258533306\n",
      "Epoch 4000/10000, MSE: 0.0016965345531813612\n",
      "Epoch 4500/10000, MSE: 0.0016803887115175607\n",
      "Epoch 5000/10000, MSE: 0.0016631871980092193\n",
      "Epoch 5500/10000, MSE: 0.0016527611131725978\n",
      "Epoch 6000/10000, MSE: 0.0016364400513694507\n",
      "Epoch 6500/10000, MSE: 0.0016293775686818137\n",
      "Epoch 7000/10000, MSE: 0.0016118823253514528\n",
      "Epoch 7500/10000, MSE: 0.0016012594278730539\n",
      "Epoch 8000/10000, MSE: 0.0015860638126704647\n",
      "Epoch 8500/10000, MSE: 0.0015771791831753988\n",
      "Epoch 9000/10000, MSE: 0.0015629125791745658\n",
      "Epoch 9500/10000, MSE: 0.0015556848176021601\n",
      "Epoch 10000/10000, MSE: 0.0015413520691408883\n",
      "Final MSE: 0.0015413520691408883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0015413520691408883)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:26.173705Z",
     "start_time": "2025-04-08T10:33:26.167865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_predict_0_v1  = neural_network_0_v1.forward(x_test_scaled_0_v1)\n",
    "y_test_mean_0_v1 = scaling_params_0_v1[2]\n",
    "y_test_sd_0_v1 = scaling_params_0_v1[3]\n",
    "y_predict_rescaled_0_v1 = y_predict_0_v1 * y_test_sd_0_v1 + y_test_mean_0_v1\n",
    "neural_network_0_v1.compute_mse(y_test_0_v1, y_predict_rescaled_0_v1)"
   ],
   "id": "d369a61c3a5cf384",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.841041693390824)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## rings3",
   "id": "5525575e701ba692"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### relu",
   "id": "91a6055b3332419a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:41:10.171733Z",
     "start_time": "2025-04-08T10:41:10.165844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(101)\n",
    "random.seed(101)"
   ],
   "id": "c00cf2fc09ae74de",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:41:17.195720Z",
     "start_time": "2025-04-08T10:41:10.512515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_1_v0 = NeuralNetwork5(layer_sizes=[2,16,16,3],\n",
    "                                     activation='relu',\n",
    "                                     output_activation='softmax',\n",
    "                                     init_method='he',\n",
    "                                     classification = True)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_1_v0, y_train_1_v0, x_test_scaled_1_v0, y_test_1_v0 =  load_and_scale_data_for_classification(rings3_train, rings3_test, target_col='c')\n",
    "\n",
    "neural_network_1_v0.train(x_train_scaled_1_v0, y_train_1_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_1_v0.train(x_train_scaled_1_v0, y_train_1_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_1_v0.train(x_train_scaled_1_v0, y_train_1_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "6605e9aa919f46e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, Loss: 46.74319884215007\n",
      "Epoch 1000/2500, Loss: 27.76033524375089\n",
      "Epoch 1500/2500, Loss: 25.627574457082996\n",
      "Epoch 2000/2500, Loss: 18.903026510877012\n",
      "Epoch 2500/2500, Loss: 16.052849185817934\n",
      "Final loss: 16.052849185817934\n",
      "Epoch 500/2500, Loss: 15.370227417247728\n",
      "Epoch 1000/2500, Loss: 15.338390075074066\n",
      "Epoch 1500/2500, Loss: 6.348556332457179\n",
      "Epoch 2000/2500, Loss: 6.799962852189053\n",
      "Epoch 2500/2500, Loss: 5.593476023241237\n",
      "Final loss: 5.593476023241237\n",
      "Epoch 500/2500, Loss: 3.9994790495256147\n",
      "Epoch 1000/2500, Loss: 2.468635284244512\n",
      "Epoch 1500/2500, Loss: 3.062095100736784\n",
      "Epoch 2000/2500, Loss: 1.922933116126045\n",
      "Epoch 2500/2500, Loss: 2.026192736517818\n",
      "Final loss: 2.026192736517818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(2.026192736517818)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5c029bba8121c9a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:41:17.208529Z",
     "start_time": "2025-04-08T10:41:17.201725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_test_1_v0 = neural_network_1_v0.forward(x_test_scaled_1_v0)\n",
    "f1_score(y_test_1_v0, y_pred_test_1_v0.argmax(axis=0), average='macro')"
   ],
   "id": "42c27ad15807ece5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713737958706292"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### tanh",
   "id": "a65c598c4c55bcd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:41:25.814655Z",
     "start_time": "2025-04-08T10:41:17.266030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_1_v1 = NeuralNetwork5(layer_sizes=[2,16,16,16,3],\n",
    "                                     activation='relu',\n",
    "                                     output_activation='softmax',\n",
    "                                     init_method='xavier',\n",
    "                                     classification = True)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_1_v1, y_train_1_v1, x_test_scaled_1_v1, y_test_1_v1 =  load_and_scale_data_for_classification(rings3_train, rings3_test, target_col='c')\n",
    "\n",
    "neural_network_1_v1.train(x_train_scaled_1_v1, y_train_1_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_1_v1.train(x_train_scaled_1_v1, y_train_1_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_1_v1.train(x_train_scaled_1_v1, y_train_1_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "3ff09280fbe99baa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, Loss: 52.243729920606434\n",
      "Epoch 1000/2500, Loss: 49.14085082015387\n",
      "Epoch 1500/2500, Loss: 34.59913557053367\n",
      "Epoch 2000/2500, Loss: 58.92111940709615\n",
      "Epoch 2500/2500, Loss: 31.87478472671765\n",
      "Final loss: 31.87478472671765\n",
      "Epoch 500/2500, Loss: 12.875688813923661\n",
      "Epoch 1000/2500, Loss: 11.592430692302644\n",
      "Epoch 1500/2500, Loss: 8.845905400213033\n",
      "Epoch 2000/2500, Loss: 8.776770953191752\n",
      "Epoch 2500/2500, Loss: 11.59128001156909\n",
      "Final loss: 11.59128001156909\n",
      "Epoch 500/2500, Loss: 1.8038827316747004\n",
      "Epoch 1000/2500, Loss: 2.7008441671961436\n",
      "Epoch 1500/2500, Loss: 0.8420645265534071\n",
      "Epoch 2000/2500, Loss: 1.384866972363933\n",
      "Epoch 2500/2500, Loss: 1.1251673160716595\n",
      "Final loss: 1.1251673160716595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.1251673160716595)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:38:52.814863Z",
     "start_time": "2025-04-08T10:38:52.808216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_test_1_v1 = neural_network_1_v1.forward(x_test_scaled_1_v1)\n",
    "f1_score(y_test_1_v1, y_pred_test_1_v1.argmax(axis=0), average='macro')"
   ],
   "id": "a46be02af8780fcb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9729209341656763"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## rings5",
   "id": "9ae0d02f2da33698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## relu",
   "id": "33a897e0e315653b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:41.559509Z",
     "start_time": "2025-04-08T10:33:41.555507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(102)\n",
    "random.seed(102)"
   ],
   "id": "717ab1d09476dcd9",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:49.111579Z",
     "start_time": "2025-04-08T10:33:41.609060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_2_v0 = NeuralNetwork5(layer_sizes=[2,16,16,16,5],\n",
    "                                     activation='relu',\n",
    "                                     output_activation='softmax',\n",
    "                                     init_method='he',\n",
    "                                     classification = True)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_2_v0, y_train_2_v0, x_test_scaled_2_v0, y_test_2_v0 =  load_and_scale_data_for_classification(rings5_train, rings5_test, target_col='c')\n",
    "\n",
    "neural_network_2_v0.train(x_train_scaled_2_v0, y_train_2_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_2_v0.train(x_train_scaled_2_v0, y_train_2_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_2_v0.train(x_train_scaled_2_v0, y_train_2_v0, batch_size = 200 ,epochs = 2500, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "1c8f59fe0174bdad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, Loss: 38.00674763980892\n",
      "Epoch 1000/2500, Loss: 44.52340957150055\n",
      "Epoch 1500/2500, Loss: 27.58043587898033\n",
      "Epoch 2000/2500, Loss: 19.75462898242037\n",
      "Epoch 2500/2500, Loss: 14.741891705307765\n",
      "Final loss: 14.741891705307765\n",
      "Epoch 500/2500, Loss: 12.508024728714442\n",
      "Epoch 1000/2500, Loss: 8.006576968194477\n",
      "Epoch 1500/2500, Loss: 13.930123475502274\n",
      "Epoch 2000/2500, Loss: 8.112924480658176\n",
      "Epoch 2500/2500, Loss: 8.51772272650754\n",
      "Final loss: 8.51772272650754\n",
      "Epoch 500/2500, Loss: 3.751397325570602\n",
      "Epoch 1000/2500, Loss: 3.2829430294041613\n",
      "Epoch 1500/2500, Loss: 2.987349946867836\n",
      "Epoch 2000/2500, Loss: 4.988641303932326\n",
      "Epoch 2500/2500, Loss: 3.671043124811834\n",
      "Final loss: 3.671043124811834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.671043124811834)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:49.172618Z",
     "start_time": "2025-04-08T10:33:49.165328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_test_2_v0 = neural_network_2_v0.forward(x_test_scaled_2_v0)\n",
    "f1_score(y_test_2_v0, y_pred_test_2_v0.argmax(axis=0), average='macro')"
   ],
   "id": "ccd7f918b1bb8779",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586758051369211"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:58.635299Z",
     "start_time": "2025-04-08T10:33:49.223795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "neural_network_2_v1 = NeuralNetwork5(layer_sizes=[2,16,16,16,5],\n",
    "                                     activation='tanh',\n",
    "                                     output_activation='softmax',\n",
    "                                     init_method='xavier',\n",
    "                                     classification = True)\n",
    "\n",
    "\n",
    "\n",
    "x_train_scaled_2_v1, y_train_2_v1, x_test_scaled_2_v1, y_test_2_v1 =  load_and_scale_data_for_classification(rings5_train, rings5_test, target_col='c')\n",
    "\n",
    "neural_network_2_v1.train(x_train_scaled_2_v1, y_train_2_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.05, verbose = 500, optimizer = 'basic')\n",
    "neural_network_2_v1.train(x_train_scaled_2_v1, y_train_2_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.025, verbose = 500, optimizer = 'basic')\n",
    "neural_network_2_v1.train(x_train_scaled_2_v1, y_train_2_v1, batch_size = 200 ,epochs = 2500, learning_rate=0.01, verbose = 500, optimizer = 'basic')\n"
   ],
   "id": "75cde56c176b4155",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/2500, Loss: 79.76938291795325\n",
      "Epoch 1000/2500, Loss: 31.79729696540621\n",
      "Epoch 1500/2500, Loss: 42.13832780918155\n",
      "Epoch 2000/2500, Loss: 26.27479503550581\n",
      "Epoch 2500/2500, Loss: 45.10630956235056\n",
      "Final loss: 45.10630956235056\n",
      "Epoch 500/2500, Loss: 16.729179215714193\n",
      "Epoch 1000/2500, Loss: 17.118789055915446\n",
      "Epoch 1500/2500, Loss: 29.25387591008485\n",
      "Epoch 2000/2500, Loss: 22.825226201249723\n",
      "Epoch 2500/2500, Loss: 17.397139228669054\n",
      "Final loss: 17.397139228669054\n",
      "Epoch 500/2500, Loss: 8.058059100124122\n",
      "Epoch 1000/2500, Loss: 8.421491037338244\n",
      "Epoch 1500/2500, Loss: 12.13255380563918\n",
      "Epoch 2000/2500, Loss: 8.122923308567758\n",
      "Epoch 2500/2500, Loss: 6.713630255584003\n",
      "Final loss: 6.713630255584003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(6.713630255584003)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:58.694829Z",
     "start_time": "2025-04-08T10:33:58.687282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_test_2_v1 = neural_network_2_v1.forward(x_test_scaled_2_v1)\n",
    "f1_score(y_test_2_v1, y_pred_test_2_v1.argmax(axis=0), average='macro')"
   ],
   "id": "d071950f12b0e3ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486007653845526"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T10:33:58.748427Z",
     "start_time": "2025-04-08T10:33:58.746539Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f44b8db1ccc30352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9007e945b5892b42"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
